"""
cleaning_diagnostic.py
=========================
è³‡æ–™æ¸…ç†è¨ºæ–·è…³æœ¬

åŠŸèƒ½ï¼š
1. è¨ºæ–· Step 2 ç•°å¸¸å€¼åˆªé™¤çš„è©³ç´°æƒ…æ³
2. åˆ†æç„¡æ³•å°æ‡‰åˆ°æ–‡åŒ–åœˆçš„åœ‹å®¶
3. ç”¢ç”Ÿè©³ç´°è¨ºæ–·å ±å‘Š

åŸ·è¡Œæ–¹å¼ï¼š
    python diagnostic/cleaning_diagnostic.py
"""

import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥è·¯å¾‘
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
from datetime import datetime

def analyze_step2_outliers(survey_df: pd.DataFrame):
    """
    åˆ†æ Step 2 ç•°å¸¸å€¼çš„è©³ç´°æƒ…æ³
    
    Parameters:
    -----------
    survey_df : pd.DataFrame
        åŸå§‹å•å·è³‡æ–™ï¼ˆç¶“éStep 1è™•ç†ï¼‰
    """
    print("\n" + "=" * 70)
    print("ã€è¨ºæ–·ã€‘Step 2: ç•°å¸¸å€¼åˆ†æ")
    print("=" * 70)
    
    outlier_details = []
    total_outliers = 0
    
    # 1. å¹´é½¡ç•°å¸¸å€¼
    if 'Review_age' in survey_df.columns:
        print("\n1ï¸âƒ£ å¹´é½¡ç•°å¸¸å€¼åˆ†æ")
        print("-" * 70)
        
        # è½‰æ›ç‚ºæ•¸å€¼
        age_numeric = pd.to_numeric(survey_df['Review_age'], errors='coerce')
        
        # çµ±è¨ˆ
        age_missing = age_numeric.isna().sum()
        age_below_18 = (age_numeric < 18).sum()
        age_above_75 = (age_numeric > 75).sum()
        age_total_outliers = age_below_18 + age_above_75
        
        print(f"   ç¼ºå¤±å€¼ï¼ˆç„¡æ³•è½‰æ›ç‚ºæ•¸å€¼ï¼‰: {age_missing:,} è¡Œ")
        print(f"   å¹´é½¡ < 18: {age_below_18:,} è¡Œ")
        print(f"   å¹´é½¡ > 75: {age_above_75:,} è¡Œ")
        print(f"   åˆè¨ˆç•°å¸¸: {age_total_outliers:,} è¡Œ")
        
        # ç•°å¸¸å€¼åˆ†ä½ˆ
        if age_below_18 > 0 or age_above_75 > 0:
            age_outliers = age_numeric[(age_numeric < 18) | (age_numeric > 75)]
            print(f"\n   ç•°å¸¸å€¼ç¯„åœ: {age_outliers.min():.1f} ~ {age_outliers.max():.1f}")
            
            # é¡¯ç¤ºå‰10å€‹æœ€æ¥µç«¯çš„å€¼
            extreme_ages = age_outliers.value_counts().head(10)
            print("\n   æœ€å¸¸å‡ºç¾çš„ç•°å¸¸å¹´é½¡:")
            for age, count in extreme_ages.items():
                print(f"      {age:.0f} æ­²: {count:,} æ¬¡")
        
        outlier_details.append({
            'é¡å‹': 'å¹´é½¡ç•°å¸¸',
            'ç¼ºå¤±å€¼': age_missing,
            '< 18æ­²': age_below_18,
            '> 75æ­²': age_above_75,
            'åˆè¨ˆ': age_total_outliers
        })
        total_outliers += age_total_outliers
    
    # 2. æ”¿æ²»ç«‹å ´ç•°å¸¸å€¼
    if 'Review_political' in survey_df.columns:
        print("\n2ï¸âƒ£ æ”¿æ²»ç«‹å ´ç•°å¸¸å€¼åˆ†æ")
        print("-" * 70)
        
        political = survey_df['Review_political']
        political_outliers = (political.notna()) & ((political < 0) | (political > 1))
        political_outlier_count = political_outliers.sum()
        
        print(f"   è¶…å‡ºç¯„åœ [0, 1]: {political_outlier_count:,} è¡Œ")
        
        if political_outlier_count > 0:
            outlier_values = political[political_outliers]
            print(f"   ç•°å¸¸å€¼ç¯„åœ: {outlier_values.min():.3f} ~ {outlier_values.max():.3f}")
            
            # é¡¯ç¤ºç•°å¸¸å€¼åˆ†ä½ˆ
            extreme_values = outlier_values.value_counts().head(10)
            print("\n   æœ€å¸¸å‡ºç¾çš„ç•°å¸¸å€¼:")
            for val, count in extreme_values.items():
                print(f"      {val:.3f}: {count:,} æ¬¡")
        
        outlier_details.append({
            'é¡å‹': 'æ”¿æ²»ç«‹å ´ç•°å¸¸',
            'è¶…å‡ºç¯„åœ': political_outlier_count,
            'åˆè¨ˆ': political_outlier_count
        })
        total_outliers += political_outlier_count
    
    # 3. å®—æ•™ç¨‹åº¦ç•°å¸¸å€¼
    if 'Review_religious' in survey_df.columns:
        print("\n3ï¸âƒ£ å®—æ•™ç¨‹åº¦ç•°å¸¸å€¼åˆ†æ")
        print("-" * 70)
        
        religious = survey_df['Review_religious']
        religious_outliers = (religious.notna()) & ((religious < 0) | (religious > 1))
        religious_outlier_count = religious_outliers.sum()
        
        print(f"   è¶…å‡ºç¯„åœ [0, 1]: {religious_outlier_count:,} è¡Œ")
        
        if religious_outlier_count > 0:
            outlier_values = religious[religious_outliers]
            print(f"   ç•°å¸¸å€¼ç¯„åœ: {outlier_values.min():.3f} ~ {outlier_values.max():.3f}")
            
            # é¡¯ç¤ºç•°å¸¸å€¼åˆ†ä½ˆ
            extreme_values = outlier_values.value_counts().head(10)
            print("\n   æœ€å¸¸å‡ºç¾çš„ç•°å¸¸å€¼:")
            for val, count in extreme_values.items():
                print(f"      {val:.3f}: {count:,} æ¬¡")
        
        outlier_details.append({
            'é¡å‹': 'å®—æ•™ç¨‹åº¦ç•°å¸¸',
            'è¶…å‡ºç¯„åœ': religious_outlier_count,
            'åˆè¨ˆ': religious_outlier_count
        })
        total_outliers += religious_outlier_count
    
    # ç¸½çµ
    print("\n" + "=" * 70)
    print("ã€ç¸½çµã€‘")
    print(f"é æœŸåˆªé™¤çš„è¡Œæ•¸: {total_outliers:,}")
    print("=" * 70)
    
    return pd.DataFrame(outlier_details)

def analyze_missing_cluster(cleaned_df: pd.DataFrame, cluster_map_df: pd.DataFrame):
    """
    åˆ†æç„¡æ³•å°æ‡‰åˆ°æ–‡åŒ–åœˆçš„åœ‹å®¶
    
    Parameters:
    -----------
    cleaned_df : pd.DataFrame
        æ¸…ç†å¾Œçš„è³‡æ–™
    cluster_map_df : pd.DataFrame
        æ–‡åŒ–åœˆåˆ†é¡è³‡æ–™
    """
    print("\n" + "=" * 70)
    print("ã€è¨ºæ–·ã€‘ç„¡æ³•å°æ‡‰æ–‡åŒ–åœˆçš„åœ‹å®¶")
    print("=" * 70)
    
    # æ‰¾å‡ºç„¡æ³•å°æ‡‰çš„è³‡æ–™
    missing_cluster = cleaned_df[cleaned_df['Cluster'].isna()]
    
    if len(missing_cluster) == 0:
        print("\nâœ… æ‰€æœ‰è³‡æ–™éƒ½æˆåŠŸå°æ‡‰åˆ°æ–‡åŒ–åœˆ")
        return None
    
    print(f"\nâš ï¸  å…±æœ‰ {len(missing_cluster):,} è¡Œç„¡æ³•å°æ‡‰")
    
    # çµ±è¨ˆå„åœ‹å®¶çš„æ•¸é‡
    missing_countries = missing_cluster['UserCountry3'].value_counts()
    
    print(f"\næ¶‰åŠ {len(missing_countries)} å€‹åœ‹å®¶:")
    print("-" * 70)
    print(f"{'åœ‹å®¶ä»£ç¢¼':<15} {'æ±ºç­–æ•¸é‡':>15} {'æ¯”ä¾‹':>15}")
    print("-" * 70)
    
    for country, count in missing_countries.items():
        pct = count / len(missing_cluster) * 100
        print(f"{country:<15} {count:>15,} {pct:>14.2f}%")
    
    # æª¢æŸ¥é€™äº›åœ‹å®¶æ˜¯å¦å­˜åœ¨æ–¼ cluster_map ä¸­
    print("\n" + "-" * 70)
    print("æª¢æŸ¥ country_cluster_map.csv ä¸­æ˜¯å¦æœ‰é€™äº›åœ‹å®¶:")
    print("-" * 70)
    
    available_countries = set(cluster_map_df['ISO3'].unique())
    
    for country in missing_countries.index:
        if country in available_countries:
            # æ‰¾å‡ºå°æ‡‰çš„è³‡æ–™
            country_info = cluster_map_df[cluster_map_df['ISO3'] == country]
            cluster = country_info['Cluster'].values[0] if len(country_info) > 0 else 'N/A'
            print(f"   {country}: âœ… å­˜åœ¨æ–¼ cluster_map (Cluster={cluster})")
        else:
            print(f"   {country}: âŒ ä¸å­˜åœ¨æ–¼ cluster_map")
    
    # å¯èƒ½çš„åŸå› åˆ†æ
    print("\n" + "-" * 70)
    print("ã€å¯èƒ½åŸå› ã€‘")
    print("-" * 70)
    print("1. åœ‹å®¶ä»£ç¢¼ä¸ä¸€è‡´ï¼ˆä¾‹å¦‚ï¼šå¤§å°å¯«ã€ç©ºç™½ï¼‰")
    print("2. cluster_map.csv ç¼ºå°‘æŸäº›åœ‹å®¶")
    print("3. è³‡æ–™åˆä½µæ™‚çš„å•é¡Œ")
    
    return missing_countries

def check_data_consistency(survey_df: pd.DataFrame, cleaned_df: pd.DataFrame):
    """
    æª¢æŸ¥è³‡æ–™ä¸€è‡´æ€§
    
    Parameters:
    -----------
    survey_df : pd.DataFrame
        åŸå§‹è³‡æ–™ï¼ˆç¶“éStep 1ï¼‰
    cleaned_df : pd.DataFrame
        æ¸…ç†å¾Œçš„è³‡æ–™
    """
    print("\n" + "=" * 70)
    print("ã€è¨ºæ–·ã€‘è³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥")
    print("=" * 70)
    
    # 1. æª¢æŸ¥ ResponseID æ˜¯å¦éƒ½æœ‰2è¡Œ
    response_counts = cleaned_df['ResponseID'].value_counts()
    
    incomplete = response_counts[response_counts != 2]
    
    if len(incomplete) == 0:
        print("\nâœ… æ‰€æœ‰å ´æ™¯éƒ½å®Œæ•´ï¼ˆæ¯å€‹ResponseIDéƒ½æœ‰2è¡Œï¼‰")
    else:
        print(f"\nâš ï¸  ç™¼ç¾ {len(incomplete)} å€‹ä¸å®Œæ•´å ´æ™¯")
        print(f"   æ¶‰åŠ {incomplete.sum()} è¡Œè³‡æ–™")
    
    # 2. æª¢æŸ¥ Saved æ¬„ä½çš„åˆ†ä½ˆ
    if 'Saved' in cleaned_df.columns:
        saved_counts = cleaned_df['Saved'].value_counts()
        print(f"\nã€Saved æ¬„ä½åˆ†ä½ˆã€‘")
        print(f"   Saved = 0 (æœªé¸æ“‡): {saved_counts.get(0, 0):,} è¡Œ")
        print(f"   Saved = 1 (é¸æ“‡): {saved_counts.get(1, 0):,} è¡Œ")
        
        # ç†è«–ä¸Šæ‡‰è©²æ˜¯1:1
        ratio = saved_counts.get(1, 0) / saved_counts.get(0, 1) if saved_counts.get(0, 0) > 0 else 0
        print(f"   æ¯”ä¾‹: {ratio:.3f} (ç†è«–ä¸Šæ‡‰æ¥è¿‘ 1.0)")
    
    # 3. æª¢æŸ¥æ–‡åŒ–åœˆåˆ†ä½ˆ
    if 'Cluster' in cleaned_df.columns:
        cluster_dist = cleaned_df['Cluster'].value_counts(dropna=False).sort_index()
        
        print(f"\nã€æ–‡åŒ–åœˆåˆ†ä½ˆã€‘")
        cluster_names = {0: 'Western', 1: 'Eastern', 2: 'Southern'}
        for cluster, count in cluster_dist.items():
            if pd.isna(cluster):
                print(f"   ç¼ºå¤±å€¼: {count:,} è¡Œ")
            else:
                cluster_name = cluster_names.get(int(cluster), f'Cluster {int(cluster)}')
                pct = count / len(cleaned_df) * 100
                print(f"   {cluster_name}: {count:,} è¡Œ ({pct:.1f}%)")

def save_diagnostic_report(outlier_df: pd.DataFrame, 
                          missing_countries: pd.Series,
                          output_dir: str = 'outputs/diagnostic'):
    """
    å„²å­˜è¨ºæ–·å ±å‘Š
    
    Parameters:
    -----------
    outlier_df : pd.DataFrame
        ç•°å¸¸å€¼çµ±è¨ˆ
    missing_countries : pd.Series
        ç„¡æ³•å°æ‡‰çš„åœ‹å®¶
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "=" * 70)
    print("å„²å­˜è¨ºæ–·å ±å‘Š...")
    print("=" * 70)
    
    # 1. å„²å­˜ç•°å¸¸å€¼çµ±è¨ˆ
    if outlier_df is not None:
        outlier_file = output_path / 'step2_outliers_detail.csv'
        outlier_df.to_csv(outlier_file, index=False, encoding='utf-8-sig')
        print(f"âœ… ç•°å¸¸å€¼è©³æƒ…: {outlier_file}")
    
    # 2. å„²å­˜ç„¡æ³•å°æ‡‰çš„åœ‹å®¶
    if missing_countries is not None:
        missing_df = missing_countries.reset_index()
        missing_df.columns = ['åœ‹å®¶ä»£ç¢¼', 'æ±ºç­–æ•¸é‡']
        
        missing_file = output_path / 'missing_cluster_countries.csv'
        missing_df.to_csv(missing_file, index=False, encoding='utf-8-sig')
        print(f"âœ… ç„¡æ³•å°æ‡‰åœ‹å®¶: {missing_file}")
    
    # 3. ç”¢ç”Ÿæ–‡å­—å ±å‘Š
    report_file = output_path / 'cleaning_diagnostic_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("è³‡æ–™æ¸…ç†è¨ºæ–·å ±å‘Š\n")
        f.write("=" * 70 + "\n")
        f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("ã€Step 2 ç•°å¸¸å€¼çµ±è¨ˆã€‘\n")
        f.write("-" * 70 + "\n")
        if outlier_df is not None:
            f.write(outlier_df.to_string(index=False))
            f.write("\n\n")
        
        f.write("ã€ç„¡æ³•å°æ‡‰æ–‡åŒ–åœˆçš„åœ‹å®¶ã€‘\n")
        f.write("-" * 70 + "\n")
        if missing_countries is not None:
            for country, count in missing_countries.items():
                f.write(f"{country}: {count:,} è¡Œ\n")
        else:
            f.write("æ‰€æœ‰åœ‹å®¶éƒ½æˆåŠŸå°æ‡‰\n")
        
        f.write("\n" + "=" * 70 + "\n")
    
    print(f"âœ… æ–‡å­—å ±å‘Š: {report_file}")
    print("=" * 70)

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "=" * 70)
    print("ğŸ” è³‡æ–™æ¸…ç†è¨ºæ–· (Diagnostic)")
    print("=" * 70)
    
    try:
        # è¼‰å…¥åŸå§‹è³‡æ–™ï¼ˆç¶“éStep 1è™•ç†ï¼‰
        print("\nã€è¼‰å…¥è³‡æ–™ã€‘")
        print("-" * 70)
        
        # å› ç‚ºæˆ‘å€‘æ²’æœ‰å„²å­˜Step 1çš„ä¸­ç¹¼è³‡æ–™ï¼Œæ‰€ä»¥é‡æ–°è¼‰å…¥ä¸¦ç°¡å–®è™•ç†
        print("è¼‰å…¥åŸå§‹å•å·è³‡æ–™...")
        survey_df = pd.read_csv('data/raw/SharedResponsesSurvey.csv', low_memory=False)
        
        # ç°¡å–®è™•ç†é—œéµè®Šæ•¸ç¼ºå¤±ï¼ˆæ¨¡æ“¬Step 1ï¼‰
        key_vars = ['Saved', 'ScenarioType', 'UserCountry3', 'ResponseID']
        survey_df = survey_df.dropna(subset=key_vars)
        print(f"âœ… è¼‰å…¥ {len(survey_df):,} è¡Œï¼ˆç¶“Step 1è™•ç†ï¼‰")
        
        # è¼‰å…¥æ¸…ç†å¾Œçš„è³‡æ–™
        print("\nè¼‰å…¥æ¸…ç†å¾Œçš„è³‡æ–™...")
        cleaned_df = pd.read_csv('data/processed/cleaned_survey.csv')
        print(f"âœ… è¼‰å…¥ {len(cleaned_df):,} è¡Œï¼ˆæœ€çµ‚è³‡æ–™ï¼‰")
        
        # è¼‰å…¥æ–‡åŒ–åœˆåˆ†é¡
        print("\nè¼‰å…¥æ–‡åŒ–åœˆåˆ†é¡...")
        cluster_map_df = pd.read_csv('data/raw/country_cluster_map.csv')
        print(f"âœ… è¼‰å…¥ {len(cluster_map_df)} å€‹åœ‹å®¶çš„åˆ†é¡")
        
        # è¨ºæ–·1: Step 2 ç•°å¸¸å€¼
        outlier_df = analyze_step2_outliers(survey_df)
        
        # è¨ºæ–·2: ç„¡æ³•å°æ‡‰æ–‡åŒ–åœˆçš„åœ‹å®¶
        missing_countries = analyze_missing_cluster(cleaned_df, cluster_map_df)
        
        # è¨ºæ–·3: è³‡æ–™ä¸€è‡´æ€§
        check_data_consistency(survey_df, cleaned_df)
        
        # å„²å­˜è¨ºæ–·å ±å‘Š
        save_diagnostic_report(outlier_df, missing_countries)
        
        print("\n" + "=" * 70)
        print("âœ… è¨ºæ–·å®Œæˆï¼")
        print("=" * 70)
        print("\nğŸ“Š å·²ç”¢ç”Ÿä»¥ä¸‹è¼¸å‡º:")
        print("  - outputs/diagnostic/step2_outliers_detail.csv")
        print("  - outputs/diagnostic/missing_cluster_countries.csv")
        print("  - outputs/diagnostic/cleaning_diagnostic_report.txt")
        print("=" * 70 + "\n")
        
    except FileNotFoundError as e:
        print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ - {e}")
        print("è«‹ç¢ºèªä»¥ä¸‹æª”æ¡ˆå­˜åœ¨:")
        print("  - data/raw/SharedResponsesSurvey.csv")
        print("  - data/processed/cleaned_survey.csv")
        print("  - data/raw/country_cluster_map.csv")
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        raise

if __name__ == '__main__':
    main()