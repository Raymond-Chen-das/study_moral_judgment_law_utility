"""
ç‰¹å¾µå·¥ç¨‹è¨ºæ–·è…³æœ¬
==================
æª¢æŸ¥ç‰¹å¾µå·¥ç¨‹åŸ·è¡Œçµæœï¼Œç‰¹åˆ¥æ˜¯ï¼š
1. Cluster == -1 çš„åˆ†ä½ˆèˆ‡ç‰¹å¾µç‹€æ…‹
2. åœ‹å®¶å±¤ç´šç‰¹å¾µçš„ç¼ºå¤±æƒ…æ³
3. è¨“ç·´/æ¸¬è©¦é›†çš„åˆ†å‰²å“è³ª
4. ä½¿ç”¨è€…å´å¯«çš„å®Œæ•´æ€§

åŸ·è¡Œæ–¹å¼ï¼š
    python diagnostic/feature_engineering_diagnostic.py
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

def check_cluster_distribution(df: pd.DataFrame) -> dict:
    """æª¢æŸ¥ Cluster åˆ†ä½ˆ"""
    print("\n" + "=" * 60)
    print("ã€1ã€‘Cluster åˆ†ä½ˆæª¢æŸ¥")
    print("=" * 60)
    
    if 'Cluster' not in df.columns:
        print("âŒ æ‰¾ä¸åˆ° Cluster æ¬„ä½")
        return {}
    
    cluster_mapping = {
        -1: 'Unclassified (æœªåˆ†é¡)',
        0: 'Western (è¥¿æ–¹)',
        1: 'Eastern (æ±æ–¹)',
        2: 'Southern (å—æ–¹)'
    }
    
    print("\nğŸ“Š Cluster åˆ†ä½ˆ:")
    cluster_counts = df['Cluster'].value_counts().sort_index()
    total_rows = len(df)
    
    results = {}
    for cluster_id, count in cluster_counts.items():
        cluster_name = cluster_mapping.get(cluster_id, f'Unknown ({cluster_id})')
        pct = count / total_rows * 100
        print(f"  {cluster_name:30s}: {count:8,} è¡Œ ({pct:5.2f}%)")
        results[cluster_id] = {'count': count, 'percentage': pct}
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ NaN
    nan_count = df['Cluster'].isna().sum()
    if nan_count > 0:
        pct = nan_count / total_rows * 100
        print(f"  {'NaN (çœŸæ­£çš„ç¼ºå¤±å€¼)':30s}: {nan_count:8,} è¡Œ ({pct:5.2f}%)")
        print("  âš ï¸  è­¦å‘Š: Cluster ä¸æ‡‰è©²æœ‰ NaN å€¼!")
    
    return results


def check_country_features_missing(df: pd.DataFrame) -> pd.DataFrame:
    """æª¢æŸ¥åœ‹å®¶å±¤ç´šç‰¹å¾µçš„ç¼ºå¤±æƒ…æ³"""
    print("\n" + "=" * 60)
    print("ã€2ã€‘åœ‹å®¶å±¤ç´šç‰¹å¾µç¼ºå¤±æª¢æŸ¥")
    print("=" * 60)
    
    # æ‰¾å‡ºæ‰€æœ‰åœ‹å®¶å±¤ç´šç‰¹å¾µ
    country_cols = [col for col in df.columns if col.startswith('country_')]
    
    if not country_cols:
        print("âŒ æ‰¾ä¸åˆ°åœ‹å®¶å±¤ç´šç‰¹å¾µ (country_* æ¬„ä½)")
        return pd.DataFrame()
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(country_cols)} å€‹åœ‹å®¶å±¤ç´šç‰¹å¾µ")
    
    # å»ºç«‹ç¼ºå¤±çµ±è¨ˆè¡¨
    missing_stats = []
    for col in country_cols:
        total = len(df)
        missing = df[col].isna().sum()
        present = total - missing
        missing_pct = missing / total * 100
        
        missing_stats.append({
            'ç‰¹å¾µåç¨±': col,
            'æœ‰æ•ˆå€¼æ•¸é‡': present,
            'ç¼ºå¤±å€¼æ•¸é‡': missing,
            'ç¼ºå¤±æ¯”ä¾‹(%)': f"{missing_pct:.2f}%"
        })
    
    missing_df = pd.DataFrame(missing_stats)
    print("\n" + missing_df.to_string(index=False))
    
    # æª¢æŸ¥ç¼ºå¤±å€¼æ˜¯å¦èˆ‡ Cluster == -1 ä¸€è‡´
    print("\n" + "-" * 60)
    print("ã€ç¼ºå¤±å€¼èˆ‡ Cluster == -1 çš„é—œä¿‚ã€‘")
    print("-" * 60)
    
    if 'Cluster' in df.columns:
        unclassified_rows = (df['Cluster'] == -1).sum()
        print(f"\nCluster == -1 çš„è¡Œæ•¸: {unclassified_rows:,}")
        
        # æª¢æŸ¥ç¬¬ä¸€å€‹åœ‹å®¶ç‰¹å¾µçš„ç¼ºå¤±æ•¸é‡
        first_country_col = country_cols[0]
        country_missing = df[first_country_col].isna().sum()
        print(f"{first_country_col} çš„ç¼ºå¤±æ•¸é‡: {country_missing:,}")
        
        if unclassified_rows == country_missing:
            print("âœ… ç¼ºå¤±æ•¸é‡èˆ‡ Cluster == -1 æ•¸é‡ä¸€è‡´")
            print("   â†’ é€™äº›åœ‹å®¶ç„¡æ³•åœ¨ CountriesChangePr.csv ä¸­æ‰¾åˆ°å°æ‡‰çš„ AMCE å€¼")
        else:
            print("âš ï¸  ç¼ºå¤±æ•¸é‡èˆ‡ Cluster == -1 æ•¸é‡ä¸ä¸€è‡´")
            print("   â†’ å¯èƒ½å­˜åœ¨å…¶ä»–åŸå› å°è‡´çš„ç¼ºå¤±")
        
        # äº¤å‰æª¢æŸ¥
        if country_missing > 0:
            # æª¢æŸ¥é€™äº›ç¼ºå¤±å€¼çš„ Cluster åˆ†ä½ˆ
            missing_mask = df[first_country_col].isna()
            missing_cluster_dist = df[missing_mask]['Cluster'].value_counts().sort_index()
            
            print("\nç¼ºå¤±å€¼çš„ Cluster åˆ†ä½ˆ:")
            for cluster_id, count in missing_cluster_dist.items():
                cluster_name = {-1: 'Unclassified', 0: 'Western', 1: 'Eastern', 2: 'Southern'}.get(cluster_id, f'Unknown')
                print(f"  Cluster {cluster_id} ({cluster_name}): {count:,} è¡Œ")
    
    return missing_df


def check_unclassified_countries(df: pd.DataFrame):
    """è©³ç´°æª¢æŸ¥ Cluster == -1 çš„åœ‹å®¶"""
    print("\n" + "=" * 60)
    print("ã€3ã€‘Unclassified åœ‹å®¶è©³ç´°æª¢æŸ¥")
    print("=" * 60)
    
    if 'Cluster' not in df.columns:
        print("âŒ æ‰¾ä¸åˆ° Cluster æ¬„ä½")
        return
    
    if 'UserCountry3' not in df.columns:
        print("âŒ æ‰¾ä¸åˆ° UserCountry3 æ¬„ä½")
        return
    
    # ç¯©é¸ Cluster == -1 çš„è³‡æ–™
    unclassified_df = df[df['Cluster'] == -1]
    
    if len(unclassified_df) == 0:
        print("âœ… æ²’æœ‰ Cluster == -1 çš„è³‡æ–™")
        return
    
    print(f"\nğŸ“Š ç¸½å…± {len(unclassified_df):,} è¡Œè³‡æ–™ (Cluster == -1)")
    
    # çµ±è¨ˆæ¶‰åŠçš„åœ‹å®¶
    country_counts = unclassified_df['UserCountry3'].value_counts()
    print(f"\næ¶‰åŠ {len(country_counts)} å€‹åœ‹å®¶:")
    print("\næ’åå‰ 20 çš„åœ‹å®¶:")
    for i, (country, count) in enumerate(country_counts.head(20).items(), 1):
        pct = count / len(unclassified_df) * 100
        print(f"  {i:2d}. {country:5s}: {count:5,} è¡Œ ({pct:5.2f}%)")
    
    if len(country_counts) > 20:
        other_count = country_counts.iloc[20:].sum()
        other_pct = other_count / len(unclassified_df) * 100
        print(f"  ... å…¶ä»– {len(country_counts)-20} å€‹åœ‹å®¶: {other_count:5,} è¡Œ ({other_pct:5.2f}%)")
    
    # æª¢æŸ¥ä½¿ç”¨è€…åˆ†ä½ˆ
    if 'UserID' in unclassified_df.columns:
        unique_users = unclassified_df['UserID'].nunique()
        print(f"\næ¶‰åŠ {unique_users:,} ä½ä½¿ç”¨è€…")


def check_scenario_features(df: pd.DataFrame):
    """æª¢æŸ¥å ´æ™¯å±¤ç´šç‰¹å¾µ"""
    print("\n" + "=" * 60)
    print("ã€4ã€‘å ´æ™¯å±¤ç´šç‰¹å¾µæª¢æŸ¥")
    print("=" * 60)
    
    scenario_features = ['is_lawful', 'is_majority', 'chose_lawful', 
                        'chose_majority', 'lawful_vs_majority_conflict']
    
    missing_features = [f for f in scenario_features if f not in df.columns]
    if missing_features:
        print(f"âŒ ç¼ºå°‘ä»¥ä¸‹ç‰¹å¾µ: {', '.join(missing_features)}")
        return
    
    print("\nğŸ“Š å ´æ™¯ç‰¹å¾µçµ±è¨ˆ:")
    stats = []
    for feat in scenario_features:
        if feat in df.columns:
            mean_val = df[feat].mean()
            std_val = df[feat].std()
            min_val = df[feat].min()
            max_val = df[feat].max()
            nan_count = df[feat].isna().sum()
            
            stats.append({
                'ç‰¹å¾µ': feat,
                'å¹³å‡å€¼': f"{mean_val:.3f}",
                'æ¨™æº–å·®': f"{std_val:.3f}",
                'ç¯„åœ': f"[{min_val:.0f}, {max_val:.0f}]",
                'ç¼ºå¤±æ•¸': nan_count
            })
    
    stats_df = pd.DataFrame(stats)
    print("\n" + stats_df.to_string(index=False))
    
    # æª¢æŸ¥é‚è¼¯ä¸€è‡´æ€§
    print("\n" + "-" * 60)
    print("ã€é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥ã€‘")
    print("-" * 60)
    
    # æª¢æŸ¥ is_lawful å’Œ is_majority çš„åˆ†ä½ˆ
    if 'is_lawful' in df.columns and 'is_majority' in df.columns:
        lawful_rate = df['is_lawful'].mean()
        majority_rate = df['is_majority'].mean()
        
        print(f"\nis_lawful ç‚º 1 çš„æ¯”ä¾‹: {lawful_rate:.1%}")
        print(f"is_majority ç‚º 1 çš„æ¯”ä¾‹: {majority_rate:.1%}")
        
        if abs(lawful_rate - 0.5) < 0.05 and abs(majority_rate - 0.5) < 0.05:
            print("âœ… åˆ†ä½ˆæ¥è¿‘ 50/50 (ç¬¦åˆé æœŸ)")
        else:
            print("âš ï¸  åˆ†ä½ˆåé›¢ 50/50 (å¯èƒ½éœ€è¦æª¢æŸ¥)")


def check_train_test_split():
    """æª¢æŸ¥è¨“ç·´/æ¸¬è©¦é›†åˆ†å‰²"""
    print("\n" + "=" * 60)
    print("ã€5ã€‘è¨“ç·´/æ¸¬è©¦é›†åˆ†å‰²æª¢æŸ¥")
    print("=" * 60)
    
    train_file = Path('data/processed/train_data.csv')
    test_file = Path('data/processed/test_data.csv')
    split_file = Path('data/processed/train_test_split.json')
    
    if not train_file.exists() or not test_file.exists():
        print("âŒ æ‰¾ä¸åˆ°è¨“ç·´/æ¸¬è©¦é›†æª”æ¡ˆ")
        return
    
    # è®€å–åˆ†å‰²è³‡è¨Š
    if split_file.exists():
        import json
        with open(split_file, 'r', encoding='utf-8') as f:
            split_info = json.load(f)
        
        train_users = set(split_info['train_users'])
        test_users = set(split_info['test_users'])
        
        print(f"\nğŸ“Š åˆ†å‰²è³‡è¨Š (ä¾†è‡ª {split_file.name}):")
        print(f"  è¨“ç·´é›†ä½¿ç”¨è€…: {len(train_users):,} ä½")
        print(f"  æ¸¬è©¦é›†ä½¿ç”¨è€…: {len(test_users):,} ä½")
        print(f"  æ¸¬è©¦é›†æ¯”ä¾‹: {split_info['test_size']:.1%}")
        print(f"  åˆ†å‰²æ™‚é–“: {split_info['split_date']}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é‡ç–Š
        overlap = train_users & test_users
        if len(overlap) > 0:
            print(f"\nâŒ ç™¼ç¾ {len(overlap)} ä½ä½¿ç”¨è€…åŒæ™‚å‡ºç¾åœ¨è¨“ç·´é›†å’Œæ¸¬è©¦é›†!")
            print("   é€™æœƒé€ æˆè³‡æ–™æ´©æ¼!")
        else:
            print("\nâœ… è¨“ç·´é›†å’Œæ¸¬è©¦é›†å®Œå…¨åˆ†é›¢ï¼Œç„¡è³‡æ–™æ´©æ¼")
    
    # è®€å–æª”æ¡ˆå¤§å°
    print("\nğŸ“Š æª”æ¡ˆè³‡è¨Š:")
    train_size_mb = train_file.stat().st_size / 1024**2
    test_size_mb = test_file.stat().st_size / 1024**2
    print(f"  è¨“ç·´é›†: {train_size_mb:.2f} MB")
    print(f"  æ¸¬è©¦é›†: {test_size_mb:.2f} MB")
    print(f"  ç¸½å¤§å°: {train_size_mb + test_size_mb:.2f} MB")


def check_user_profiles():
    """æª¢æŸ¥ä½¿ç”¨è€…é“å¾·å´å¯«"""
    print("\n" + "=" * 60)
    print("ã€6ã€‘ä½¿ç”¨è€…é“å¾·å´å¯«æª¢æŸ¥")
    print("=" * 60)
    
    profile_file = Path('data/processed/user_moral_profiles.csv')
    
    if not profile_file.exists():
        print("âŒ æ‰¾ä¸åˆ°ä½¿ç”¨è€…å´å¯«æª”æ¡ˆ")
        return
    
    profiles_df = pd.read_csv(profile_file)
    print(f"\nğŸ“Š ç¸½å…± {len(profiles_df):,} ä½ä½¿ç”¨è€…")
    
    # æª¢æŸ¥åˆ†å‰²æ¨™è¨˜
    if 'split' in profiles_df.columns:
        train_count = (profiles_df['split'] == 'train').sum()
        test_count = (profiles_df['split'] == 'test').sum()
        
        print(f"\nåˆ†å‰²æƒ…æ³:")
        print(f"  è¨“ç·´é›†: {train_count:,} ä½ ({train_count/len(profiles_df)*100:.1f}%)")
        print(f"  æ¸¬è©¦é›†: {test_count:,} ä½ ({test_count/len(profiles_df)*100:.1f}%)")
        
        if abs(train_count / len(profiles_df) - 0.8) < 0.02:
            print("  âœ… åˆ†å‰²æ¯”ä¾‹æ¥è¿‘ 80/20")
        else:
            print("  âš ï¸  åˆ†å‰²æ¯”ä¾‹åé›¢ 80/20")
    
    # æª¢æŸ¥å´å¯«ç‰¹å¾µ
    profile_features = ['utilitarian_score', 'deontology_score', 
                       'consistency_score', 'n_scenarios']
    
    print("\nğŸ“Š å´å¯«ç‰¹å¾µçµ±è¨ˆ:")
    for feat in profile_features:
        if feat in profiles_df.columns:
            mean_val = profiles_df[feat].mean()
            std_val = profiles_df[feat].std()
            min_val = profiles_df[feat].min()
            max_val = profiles_df[feat].max()
            
            print(f"\n{feat}:")
            print(f"  å¹³å‡: {mean_val:.3f}")
            print(f"  æ¨™æº–å·®: {std_val:.3f}")
            print(f"  ç¯„åœ: [{min_val:.3f}, {max_val:.3f}]")
    
    # æª¢æŸ¥é“å¾·å‚¾å‘åˆ†ä½ˆ
    if 'utilitarian_score' in profiles_df.columns:
        print("\nğŸ“Š é“å¾·å‚¾å‘åˆ†ä½ˆ:")
        strong_util = (profiles_df['utilitarian_score'] > 0.7).sum()
        moderate = ((profiles_df['utilitarian_score'] >= 0.3) & 
                   (profiles_df['utilitarian_score'] <= 0.7)).sum()
        strong_deont = (profiles_df['utilitarian_score'] < 0.3).sum()
        
        print(f"  å¼·æ•ˆç›Šä¸»ç¾© (>0.7): {strong_util:,} ä½ ({strong_util/len(profiles_df)*100:.1f}%)")
        print(f"  ä¸­é–“æ´¾ (0.3-0.7): {moderate:,} ä½ ({moderate/len(profiles_df)*100:.1f}%)")
        print(f"  å¼·ç¾©å‹™è«– (<0.3): {strong_deont:,} ä½ ({strong_deont/len(profiles_df)*100:.1f}%)")


def generate_diagnostic_summary(df: pd.DataFrame):
    """ç”Ÿæˆè¨ºæ–·æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ã€è¨ºæ–·æ‘˜è¦ã€‘")
    print("=" * 60)
    
    issues = []
    warnings = []
    success = []
    
    # æª¢æŸ¥ Cluster
    if 'Cluster' in df.columns:
        if df['Cluster'].isna().sum() > 0:
            issues.append("Cluster æ¬„ä½æœ‰ NaN å€¼")
        else:
            success.append("Cluster æ¬„ä½ç„¡ç¼ºå¤±å€¼")
        
        unclassified_count = (df['Cluster'] == -1).sum()
        if unclassified_count > 0:
            warnings.append(f"{unclassified_count:,} è¡Œå±¬æ–¼ Unclassified (é€™æ˜¯æ­£å¸¸çš„)")
    
    # æª¢æŸ¥åœ‹å®¶ç‰¹å¾µ
    country_cols = [col for col in df.columns if col.startswith('country_')]
    if country_cols:
        first_country_col = country_cols[0]
        missing_count = df[first_country_col].isna().sum()
        if missing_count > 0:
            warnings.append(f"åœ‹å®¶ç‰¹å¾µæœ‰ {missing_count:,} å€‹ç¼ºå¤±å€¼ (å°æ‡‰ Cluster == -1)")
        else:
            success.append("åœ‹å®¶ç‰¹å¾µç„¡ç¼ºå¤±å€¼")
    
    # è¼¸å‡ºæ‘˜è¦
    print("\nâœ… æˆåŠŸé …ç›®:")
    for item in success:
        print(f"  â€¢ {item}")
    
    if warnings:
        print("\nâš ï¸  æ³¨æ„äº‹é …:")
        for item in warnings:
            print(f"  â€¢ {item}")
    
    if issues:
        print("\nâŒ ç™¼ç¾å•é¡Œ:")
        for item in issues:
            print(f"  â€¢ {item}")
    else:
        print("\nğŸ‰ æœªç™¼ç¾åš´é‡å•é¡Œ")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "=" * 80)
    print("ğŸ” MIT Moral Machine - ç‰¹å¾µå·¥ç¨‹è¨ºæ–·å ±å‘Š")
    print("=" * 80)
    
    # è¼‰å…¥è³‡æ–™
    featured_file = Path('data/processed/featured_data.csv')
    
    if not featured_file.exists():
        print(f"\nâŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {featured_file}")
        print("è«‹å…ˆåŸ·è¡Œ 03_feature_engineering.py")
        return
    
    print(f"\nğŸ“‚ è¼‰å…¥è³‡æ–™: {featured_file}")
    df = pd.read_csv(featured_file)
    print(f"   ç¸½è¡Œæ•¸: {len(df):,}")
    print(f"   ç¸½æ¬„ä½: {len(df.columns)}")
    
    # åŸ·è¡Œå„é …æª¢æŸ¥
    check_cluster_distribution(df)
    missing_df = check_country_features_missing(df)
    check_unclassified_countries(df)
    check_scenario_features(df)
    check_train_test_split()
    check_user_profiles()
    generate_diagnostic_summary(df)
    
    # å„²å­˜è¨ºæ–·çµæœ
    output_dir = Path('outputs/diagnostic')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if not missing_df.empty:
        output_file = output_dir / 'country_features_missing_report.csv'
        missing_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ è¨ºæ–·å ±å‘Šå·²å„²å­˜: {output_file}")
    
    print("\n" + "=" * 80)
    print("âœ… è¨ºæ–·å®Œæˆ")
    print("=" * 80 + "\n")


if __name__ == '__main__':
    main()