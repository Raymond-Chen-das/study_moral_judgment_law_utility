"""
æ½›åœ¨é¡åˆ¥åˆ†ææ¨¡çµ„
================
è² è²¬ç¬¬3ç«  3.4ç¯€çš„åˆ†æï¼šé“å¾·äººæ ¼é¡å‹å­¸

åŠŸèƒ½ï¼š
1. å»ºç«‹ä½¿ç”¨è€…é“å¾·å´å¯«ï¼ˆåŸºæ–¼è¨“ç·´é›†ï¼‰
2. ä½¿ç”¨Gaussian Mixture Modelé€²è¡Œæ½›åœ¨é¡åˆ¥åˆ†æ
3. BICæ›²ç·šé¸æ“‡æœ€ä½³é¡åˆ¥æ•¸
4. é¡åˆ¥ç‰¹å¾µé›·é”åœ–
5. é¡åˆ¥åœ¨æ–‡åŒ–åœˆçš„åˆ†ä½ˆ
"""

import pandas as pd
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from pathlib import Path
from typing import Dict, Tuple
import logging


class LatentClassAnalyzer:
    """æ½›åœ¨é¡åˆ¥åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """è¨­å®šæ—¥èªŒè¨˜éŒ„å™¨"""
        logger = logging.getLogger('LatentClassAnalyzer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)
        
        return logger
    
    def load_user_profiles(self, filepath: str = 'data/processed/user_moral_profiles.csv') -> pd.DataFrame:
        """
        è¼‰å…¥ä½¿ç”¨è€…é“å¾·å´å¯«
        
        Parameters:
        -----------
        filepath : str
            ä½¿ç”¨è€…å´å¯«æª”æ¡ˆè·¯å¾‘
            
        Returns:
        --------
        pd.DataFrame
            ä½¿ç”¨è€…å´å¯«è³‡æ–™ï¼ˆåƒ…è¨“ç·´é›†ï¼‰
        """
        self.logger.info(f"è¼‰å…¥ä½¿ç”¨è€…é“å¾·å´å¯«: {filepath}")
        profiles_df = pd.read_csv(filepath)
        
        # åƒ…ä½¿ç”¨è¨“ç·´é›†ï¼ˆé¿å…è³‡æ–™æ´©æ¼ï¼‰
        if 'split' in profiles_df.columns:
            train_profiles = profiles_df[profiles_df['split'] == 'train'].copy()
            self.logger.info(f"åƒ…ä½¿ç”¨è¨“ç·´é›†: {len(train_profiles):,} ä½ä½¿ç”¨è€…")
        else:
            train_profiles = profiles_df.copy()
            self.logger.info(f"ä½¿ç”¨å…¨éƒ¨è³‡æ–™: {len(train_profiles):,} ä½ä½¿ç”¨è€…")
        
        return train_profiles
    
    def prepare_features(self, profiles_df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:
        """
        æº–å‚™ç‰¹å¾µçŸ©é™£
        
        Parameters:
        -----------
        profiles_df : pd.DataFrame
            ä½¿ç”¨è€…å´å¯«
            
        Returns:
        --------
        Tuple[pd.DataFrame, np.ndarray]
            (åŸå§‹ç‰¹å¾µ, æ¨™æº–åŒ–ç‰¹å¾µ)
        """
        self.logger.info("æº–å‚™ç‰¹å¾µçŸ©é™£...")
        
        # é¸æ“‡ç‰¹å¾µï¼ˆæ•ˆç›Šä¸»ç¾©å’Œç¾©å‹™è«–åˆ†æ•¸ï¼‰
        feature_cols = ['utilitarian_score', 'deontology_score']
        
        features_df = profiles_df[feature_cols].copy()
        
        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features_df)
        
        self.logger.info(f"ç‰¹å¾µæº–å‚™å®Œæˆ: {features_df.shape}")
        
        return features_df, features_scaled
    
    def find_optimal_clusters(self,
                            features_scaled: np.ndarray,
                            k_range: range = range(2, 7)) -> Tuple[int, Dict]:
        """
        ä½¿ç”¨BICæ‰¾å‡ºæœ€ä½³é¡åˆ¥æ•¸
        
        Parameters:
        -----------
        features_scaled : np.ndarray
            æ¨™æº–åŒ–ç‰¹å¾µ
        k_range : range
            æ¸¬è©¦çš„é¡åˆ¥æ•¸ç¯„åœ
            
        Returns:
        --------
        Tuple[int, Dict]
            (æœ€ä½³k, BICåˆ†æ•¸å­—å…¸)
        """
        self.logger.info(f"æ¸¬è©¦é¡åˆ¥æ•¸: {list(k_range)}")
        
        bic_scores = {}
        
        for k in k_range:
            gmm = GaussianMixture(
                n_components=k,
                covariance_type='full',
                random_state=42,
                n_init=10
            )
            gmm.fit(features_scaled)
            bic_scores[k] = gmm.bic(features_scaled)
            
            self.logger.info(f"  k={k}: BIC={bic_scores[k]:.2f}")
        
        # æ‰¾å‡ºBICæœ€å°çš„k
        optimal_k = min(bic_scores, key=bic_scores.get)
        
        self.logger.info(f"âœ… æœ€ä½³é¡åˆ¥æ•¸: {optimal_k} (BIC={bic_scores[optimal_k]:.2f})")
        
        return optimal_k, bic_scores
    
    def fit_gmm(self,
               features_scaled: np.ndarray,
               n_clusters: int) -> GaussianMixture:
        """
        è¨“ç·´GMMæ¨¡å‹
        
        Parameters:
        -----------
        features_scaled : np.ndarray
            æ¨™æº–åŒ–ç‰¹å¾µ
        n_clusters : int
            é¡åˆ¥æ•¸
            
        Returns:
        --------
        GaussianMixture
            è¨“ç·´å¥½çš„æ¨¡å‹
        """
        self.logger.info(f"è¨“ç·´GMMæ¨¡å‹ (k={n_clusters})...")
        
        gmm = GaussianMixture(
            n_components=n_clusters,
            covariance_type='full',
            random_state=42,
            n_init=10
        )
        gmm.fit(features_scaled)
        
        self.logger.info("æ¨¡å‹è¨“ç·´å®Œæˆ")
        
        return gmm
    
    def create_bic_curve(self,
                        bic_scores: Dict,
                        optimal_k: int,
                        output_path: str = 'outputs/figures/chapter3_exploration/lca_bic_curve.html') -> str:
        """
        å»ºç«‹BICæ›²ç·šåœ–
        
        Parameters:
        -----------
        bic_scores : Dict
            BICåˆ†æ•¸å­—å…¸
        optimal_k : int
            æœ€ä½³kå€¼
        output_path : str
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            
        Returns:
        --------
        str
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        self.logger.info("å»ºç«‹BICæ›²ç·šåœ–...")
        
        k_values = list(bic_scores.keys())
        bic_values = list(bic_scores.values())
        
        fig = go.Figure()
        
        # BICæ›²ç·š
        fig.add_trace(go.Scatter(
            x=k_values,
            y=bic_values,
            mode='lines+markers',
            name='BIC',
            line=dict(color='blue', width=2),
            marker=dict(size=10)
        ))
        
        # æ¨™è¨˜æœ€ä½³k
        fig.add_trace(go.Scatter(
            x=[optimal_k],
            y=[bic_scores[optimal_k]],
            mode='markers',
            name=f'æœ€ä½³ k={optimal_k}',
            marker=dict(color='red', size=15, symbol='star')
        ))
        
        fig.update_layout(
            title='BICæ›²ç·šï¼šæ½›åœ¨é¡åˆ¥æ•¸é¸æ“‡',
            xaxis_title='é¡åˆ¥æ•¸ (k)',
            yaxis_title='BICåˆ†æ•¸ï¼ˆè¶Šå°è¶Šå¥½ï¼‰',
            font=dict(family="Arial, sans-serif", size=14),
            title_font_size=20,
            title_x=0.5,
            height=500,
            showlegend=True
        )
        
        # å„²å­˜
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.write_html(output_path)
        self.logger.info(f"BICæ›²ç·šå·²å„²å­˜: {output_path}")
        
        return output_path
    
    def analyze_class_profiles(self,
                               profiles_df: pd.DataFrame,
                               features_df: pd.DataFrame,
                               labels: np.ndarray) -> pd.DataFrame:
        """
        åˆ†æå„é¡åˆ¥çš„ç‰¹å¾µ
        
        Parameters:
        -----------
        profiles_df : pd.DataFrame
            ä½¿ç”¨è€…å´å¯«
        features_df : pd.DataFrame
            åŸå§‹ç‰¹å¾µ
        labels : np.ndarray
            é¡åˆ¥æ¨™ç±¤
            
        Returns:
        --------
        pd.DataFrame
            é¡åˆ¥ç‰¹å¾µè¡¨
        """
        self.logger.info("åˆ†æé¡åˆ¥ç‰¹å¾µ...")
        
        # åŠ ä¸Šé¡åˆ¥æ¨™ç±¤
        profiles_with_class = profiles_df.copy()
        profiles_with_class['class'] = labels
        
        # è¨ˆç®—å„é¡åˆ¥çš„å¹³å‡å€¼
        class_profiles = profiles_with_class.groupby('class').agg({
            'utilitarian_score': 'mean',
            'deontology_score': 'mean',
            'consistency_score': 'mean',
            'n_scenarios': 'mean'
        }).reset_index()
        
        # åŠ ä¸Šæ¨£æœ¬æ•¸
        class_profiles['n_users'] = profiles_with_class.groupby('class').size().values
        class_profiles['percentage'] = (class_profiles['n_users'] / len(profiles_with_class) * 100).round(1)
        
        # é¡åˆ¥å‘½åï¼ˆåŸºæ–¼ç‰¹å¾µï¼‰
        class_names = []
        for _, row in class_profiles.iterrows():
            if row['utilitarian_score'] > 0.7:
                name = 'å¼·æ•ˆç›Šä¸»ç¾©è€…'
            elif row['deontology_score'] > 0.7:
                name = 'å¼·ç¾©å‹™è«–è€…'
            elif abs(row['utilitarian_score'] - 0.5) < 0.2:
                name = 'ä¸­é–“æ´¾'
            else:
                name = f"é¡åˆ¥{int(row['class'])}"
            class_names.append(name)
        
        class_profiles['class_name'] = class_names
        
        self.logger.info("\né¡åˆ¥ç‰¹å¾µ:")
        for _, row in class_profiles.iterrows():
            self.logger.info(f"  {row['class_name']}: {row['n_users']:,} ä½ ({row['percentage']:.1f}%)")
            self.logger.info(f"    æ•ˆç›Šä¸»ç¾©: {row['utilitarian_score']:.3f}, ç¾©å‹™è«–: {row['deontology_score']:.3f}")
        
        return class_profiles
    
    def create_class_radar(self,
                          class_profiles: pd.DataFrame,
                          output_path: str = 'outputs/figures/chapter3_exploration/lca_class_profiles.html') -> str:
        """
        å»ºç«‹é¡åˆ¥ç‰¹å¾µé›·é”åœ–
        
        Parameters:
        -----------
        class_profiles : pd.DataFrame
            é¡åˆ¥ç‰¹å¾µ
        output_path : str
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            
        Returns:
        --------
        str
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        self.logger.info("å»ºç«‹é¡åˆ¥ç‰¹å¾µé›·é”åœ–...")
        
        fig = go.Figure()
        
        dimensions = ['utilitarian_score', 'deontology_score', 'consistency_score']
        dimension_labels = ['æ•ˆç›Šä¸»ç¾©', 'ç¾©å‹™è«–', 'ä¸€è‡´æ€§']
        
        colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']
        
        for idx, row in class_profiles.iterrows():
            values = [row[dim] for dim in dimensions]
            
            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=dimension_labels,
                fill='toself',
                name=f"{row['class_name']} ({row['percentage']:.1f}%)",
                line=dict(color=colors[idx % len(colors)], width=2),
                opacity=0.6
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )
            ),
            showlegend=True,
            title='æ½›åœ¨é¡åˆ¥ç‰¹å¾µé›·é”åœ–',
            font=dict(family="Arial, sans-serif", size=14),
            title_font_size=20,
            title_x=0.5,
            height=600
        )
        
        # å„²å­˜
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.write_html(output_path)
        self.logger.info(f"é¡åˆ¥é›·é”åœ–å·²å„²å­˜: {output_path}")
        
        return output_path
    
    def analyze_class_by_culture(self,
                                 profiles_df: pd.DataFrame,
                                 labels: np.ndarray,
                                 df: pd.DataFrame,
                                 output_path: str = 'outputs/figures/chapter3_exploration/lca_culture_distribution.html') -> str:
        """
        åˆ†æé¡åˆ¥åœ¨æ–‡åŒ–åœˆçš„åˆ†ä½ˆ
        
        Parameters:
        -----------
        profiles_df : pd.DataFrame
            ä½¿ç”¨è€…å´å¯«
        labels : np.ndarray
            é¡åˆ¥æ¨™ç±¤
        df : pd.DataFrame
            åŸå§‹è³‡æ–™ï¼ˆåŒ…å«Clusterè³‡è¨Šï¼‰
        output_path : str
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            
        Returns:
        --------
        str
            è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        self.logger.info("åˆ†æé¡åˆ¥èˆ‡æ–‡åŒ–åœˆçš„é—œä¿‚...")
        
        # å–å¾—æ¯ä½ä½¿ç”¨è€…çš„æ–‡åŒ–åœˆ
        user_cluster = df.groupby('UserID')['Cluster'].first()
        
        # åˆä½µ
        profiles_with_class = profiles_df.copy()
        profiles_with_class['class'] = labels
        profiles_with_class['cluster'] = profiles_with_class['UserID'].map(user_cluster)
        
        # ç§»é™¤æ²’æœ‰clusterè³‡è¨Šçš„
        profiles_with_class = profiles_with_class.dropna(subset=['cluster'])
        
        # äº¤å‰è¡¨
        crosstab = pd.crosstab(
            profiles_with_class['class'],
            profiles_with_class['cluster'],
            normalize='index'
        ) * 100
        
        # å»ºç«‹å †ç–Šé•·æ¢åœ–
        cluster_names = {0: 'Western', 1: 'Eastern', 2: 'Southern'}
        
        fig = go.Figure()
        
        for cluster_id in sorted(crosstab.columns):
            fig.add_trace(go.Bar(
                name=cluster_names[cluster_id],
                x=crosstab.index,
                y=crosstab[cluster_id],
                text=crosstab[cluster_id].round(1).astype(str) + '%',
                textposition='inside'
            ))
        
        fig.update_layout(
            barmode='stack',
            title='å„é¡åˆ¥åœ¨æ–‡åŒ–åœˆçš„åˆ†ä½ˆ',
            xaxis_title='é¡åˆ¥',
            yaxis_title='ç™¾åˆ†æ¯” (%)',
            font=dict(family="Arial, sans-serif", size=14),
            title_font_size=20,
            title_x=0.5,
            height=500,
            showlegend=True,
            yaxis_ticksuffix='%'
        )
        
        # å„²å­˜
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.write_html(output_path)
        self.logger.info(f"æ–‡åŒ–åˆ†ä½ˆåœ–å·²å„²å­˜: {output_path}")
        
        return output_path
    
    def run_analysis(self, df: pd.DataFrame) -> Dict[str, any]:
        """
        åŸ·è¡Œå®Œæ•´çš„æ½›åœ¨é¡åˆ¥åˆ†æ
        
        Parameters:
        -----------
        df : pd.DataFrame
            åŸå§‹è³‡æ–™
            
        Returns:
        --------
        Dict[str, any]
            åˆ†æçµæœå­—å…¸
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("é–‹å§‹æ½›åœ¨é¡åˆ¥åˆ†æ...")
        self.logger.info("=" * 60)
        
        # 1. è¼‰å…¥ä½¿ç”¨è€…å´å¯«
        profiles_df = self.load_user_profiles()
        
        # 2. æº–å‚™ç‰¹å¾µ
        features_df, features_scaled = self.prepare_features(profiles_df)
        
        # 3. æ‰¾å‡ºæœ€ä½³é¡åˆ¥æ•¸
        optimal_k, bic_scores = self.find_optimal_clusters(features_scaled)
        
        # 4. å»ºç«‹BICæ›²ç·š
        bic_path = self.create_bic_curve(bic_scores, optimal_k)
        
        # 5. è¨“ç·´æœ€ä½³æ¨¡å‹
        gmm = self.fit_gmm(features_scaled, optimal_k)
        labels = gmm.predict(features_scaled)
        
        # 6. åˆ†æé¡åˆ¥ç‰¹å¾µ
        class_profiles = self.analyze_class_profiles(profiles_df, features_df, labels)
        
        # 7. å»ºç«‹é¡åˆ¥é›·é”åœ–
        radar_path = self.create_class_radar(class_profiles)
        
        # 8. åˆ†æé¡åˆ¥èˆ‡æ–‡åŒ–çš„é—œä¿‚
        culture_path = self.analyze_class_by_culture(profiles_df, labels, df)
        
        # 9. å„²å­˜çµæœ
        output_dir = Path('outputs/tables/chapter3')
        output_dir.mkdir(parents=True, exist_ok=True)
        
        class_profiles.to_csv(output_dir / 'lca_class_profiles.csv', index=False, encoding='utf-8-sig')
        
        # å„²å­˜BICåˆ†æ•¸
        bic_df = pd.DataFrame({
            'k': list(bic_scores.keys()),
            'BIC': list(bic_scores.values())
        })
        bic_df.to_csv(output_dir / 'lca_bic_scores.csv', index=False, encoding='utf-8-sig')
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("æ½›åœ¨é¡åˆ¥åˆ†æå®Œæˆï¼")
        self.logger.info("=" * 60)
        
        return {
            'bic_curve': bic_path,
            'class_radar': radar_path,
            'culture_distribution': culture_path,
            'optimal_k': optimal_k,
            'class_profiles': class_profiles
        }
    def run_sensitivity_analysis(self, profiles_df: pd.DataFrame = None) -> Dict:
        """
        åŸ·è¡Œæ•æ„Ÿåº¦åˆ†æï¼šæª¢é©—ã€Œæ¥µç«¯åˆ†æ•¸ã€æ˜¯å¦ç‚ºæ¸¬é‡å‡è±¡
        
        ã€æ–°å¢æ–¹æ³•ã€‘æ­¤æ–¹æ³•ç‚º3.4ç¯€çš„è£œå……åˆ†æï¼Œä¸å½±éŸ¿åŸæœ‰çš„run_analysis()
        
        Parameters:
        -----------
        profiles_df : pd.DataFrame, optional
            ä½¿ç”¨è€…å´å¯«è³‡æ–™ã€‚å¦‚æœç‚ºNoneï¼Œå‰‡è‡ªå‹•è¼‰å…¥
            
        Returns:
        --------
        Dict
            æ•æ„Ÿåº¦åˆ†æçµæœ
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("é–‹å§‹LCAæ•æ„Ÿåº¦åˆ†æ...")
        self.logger.info("=" * 60)
        
        # è¼‰å…¥è³‡æ–™ï¼ˆå¦‚æœæ²’æœ‰æä¾›ï¼‰
        if profiles_df is None:
            profiles_df = self.load_user_profiles()
        
        # ä¾å ´æ™¯æ•¸é‡åˆ†å±¤
        self.logger.info("ä¾å ´æ™¯æ•¸é‡åˆ†å±¤...")
        
        strata = {
            '1æ¬¡': profiles_df[profiles_df['n_scenarios'] == 1].copy(),
            '2æ¬¡': profiles_df[profiles_df['n_scenarios'] == 2].copy(),
            '3æ¬¡ä»¥ä¸Š': profiles_df[profiles_df['n_scenarios'] >= 3].copy()
        }
        
        stratified_results = {}
        
        for stratum_name, stratum_df in strata.items():
            if len(stratum_df) == 0:
                self.logger.warning(f"{stratum_name} ç„¡è³‡æ–™")
                continue
            
            # è¨ˆç®—æ¥µç«¯æ¯”ä¾‹
            extreme_mask = stratum_df['utilitarian_score'].isin([0, 1])
            extreme_ratio = extreme_mask.mean()
            
            stratified_results[stratum_name] = {
                'n_users': len(stratum_df),
                'extreme_ratio': extreme_ratio,
                'consistency_mean': stratum_df['consistency_score'].mean(),
                'consistency_std': stratum_df['consistency_score'].std(),
                'util_mean': stratum_df['utilitarian_score'].mean(),
                'util_std': stratum_df['utilitarian_score'].std()
            }
            
            self.logger.info(f"\nã€{stratum_name}ã€‘")
            self.logger.info(f"  ä½¿ç”¨è€…æ•¸: {len(stratum_df):,}")
            self.logger.info(f"  æ¥µç«¯æ¯”ä¾‹: {extreme_ratio:.1%}")
        
        # å»ºç«‹æ¥µç«¯æ¯”ä¾‹æ¯”è¼ƒåœ–
        extreme_plot = self._create_extreme_ratio_plot(stratified_results)
        
        # çµ±è¨ˆæª¢å®š
        test_result = self._perform_chi_square_test(stratified_results, profiles_df)
        
        # ç”Ÿæˆè©®é‡‹
        interpretation = self._generate_sensitivity_interpretation(stratified_results)
        
        # å„²å­˜çµæœ
        output_dir = Path('outputs/tables/chapter3')
        output_dir.mkdir(parents=True, exist_ok=True)
        
        summary_df = pd.DataFrame([
            {
                'è§€æ¸¬æ¬¡æ•¸': name,
                'ä½¿ç”¨è€…æ•¸': results['n_users'],
                'æ¥µç«¯æ¯”ä¾‹': results['extreme_ratio'],
                'ä¸€è‡´æ€§å‡å€¼': results['consistency_mean'],
                'æ•ˆç›Šä¸»ç¾©å‡å€¼': results['util_mean']
            }
            for name, results in stratified_results.items()
        ])
        
        summary_df.to_csv(
            output_dir / 'lca_sensitivity_summary.csv',
            index=False,
            encoding='utf-8-sig'
        )
        
        test_result.to_csv(
            output_dir / 'lca_sensitivity_chi_square.csv',
            index=False,
            encoding='utf-8-sig'
        )
        
        with open(output_dir / 'lca_sensitivity_interpretation.md', 'w', encoding='utf-8') as f:
            f.write(interpretation)
        
        self.logger.info("\n" + interpretation)
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("æ•æ„Ÿåº¦åˆ†æå®Œæˆï¼")
        self.logger.info("=" * 60)
        
        return {
            'extreme_plot': extreme_plot,
            'test_result': test_result,
            'interpretation': interpretation,
            'stratified_results': stratified_results
        }

    def _create_extreme_ratio_plot(self, stratified_results: Dict) -> str:
        """å»ºç«‹æ¥µç«¯æ¯”ä¾‹æ¯”è¼ƒåœ–"""
        strata_names = list(stratified_results.keys())
        extreme_ratios = [stratified_results[s]['extreme_ratio'] for s in strata_names]
        n_users = [stratified_results[s]['n_users'] for s in strata_names]
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            x=strata_names,
            y=extreme_ratios,
            text=[f'{r:.1%}<br>n={n:,}' for r, n in zip(extreme_ratios, n_users)],
            textposition='outside',
            marker=dict(
                color=extreme_ratios,
                colorscale='RdYlGn_r',
                showscale=True,
                colorbar=dict(title='æ¥µç«¯æ¯”ä¾‹')
            )
        ))
        
        fig.update_layout(
            title='æ•æ„Ÿåº¦åˆ†æï¼šä¸åŒè§€æ¸¬æ¬¡æ•¸çš„æ¥µç«¯æ¯”ä¾‹',
            xaxis_title='è§€æ¸¬æ¬¡æ•¸',
            yaxis_title='æ¥µç«¯åˆ†æ•¸æ¯”ä¾‹ï¼ˆ0æˆ–1ï¼‰',
            yaxis_tickformat='.0%',
            yaxis_range=[0, 1.1],
            font=dict(family="Arial, sans-serif", size=14),
            title_font_size=20,
            title_x=0.5,
            height=500
        )
        
        fig.add_hline(y=0.9, line_dash="dash", line_color="red", 
                    annotation_text="90% é–¾å€¼", annotation_position="right")
        
        output_path = 'outputs/figures/chapter3_exploration/lca_sensitivity_extreme_ratio.html'
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.write_html(output_path)
        
        self.logger.info(f"æ¥µç«¯æ¯”ä¾‹åœ–å·²å„²å­˜: {output_path}")
        
        return output_path

    def _perform_chi_square_test(self, stratified_results: Dict, profiles_df: pd.DataFrame) -> pd.DataFrame:
        """åŸ·è¡Œå¡æ–¹æª¢å®š"""
        from scipy.stats import chi2_contingency
        
        contingency_data = []
        for name, results in stratified_results.items():
            if '1æ¬¡' in name:
                data = profiles_df[profiles_df['n_scenarios'] == 1]
            elif '2æ¬¡' in name:
                data = profiles_df[profiles_df['n_scenarios'] == 2]
            else:  # 3æ¬¡ä»¥ä¸Š
                data = profiles_df[profiles_df['n_scenarios'] >= 3]
            
            if len(data) > 0:
                n_extreme = (data['utilitarian_score'].isin([0, 1])).sum()
                n_non_extreme = len(data) - n_extreme
                contingency_data.append([n_extreme, n_non_extreme])
        
        if len(contingency_data) >= 2:
            contingency_table = np.array(contingency_data)
            chi2, p_value, dof, expected = chi2_contingency(contingency_table)
            
            self.logger.info(f"\nã€å¡æ–¹æª¢å®šã€‘Ï‡Â² = {chi2:.2f}, p = {p_value:.4f}")
            
            return pd.DataFrame({
                'æª¢å®šé¡å‹': ['å¡æ–¹æª¢å®š'],
                'Ï‡Â²å€¼': [chi2],
                'på€¼': [p_value],
                'è‡ªç”±åº¦': [dof],
                'é¡¯è‘—æ€§': ['***' if p_value < 0.001 else ('*' if p_value < 0.05 else 'ns')]
            })
        else:
            return pd.DataFrame()

    def _generate_sensitivity_interpretation(self, stratified_results: Dict) -> str:
        """ç”Ÿæˆè©®é‡‹"""
        extreme_ratios = {k: v['extreme_ratio'] for k, v in stratified_results.items()}
        
        interpretation = []
        interpretation.append("## æ•æ„Ÿåº¦åˆ†æï¼šè©®é‡‹\n")
        
        three_plus = extreme_ratios.get('3æ¬¡ä»¥ä¸Š', 0)
        
        if three_plus > 0.90:
            interpretation.append("### âœ… æ”¯æŒã€ŒçœŸå¯¦æ±ºç­–æ¨¡å¼ã€å‡è¨­\n")
            interpretation.append(f"- å³ä½¿å®Œæˆ3æ¬¡ä»¥ä¸Šå ´æ™¯ï¼Œæ¥µç«¯æ¯”ä¾‹ä»é«˜é” {three_plus:.1%}\n")
            interpretation.append("- è¡¨ç¤ºä½¿ç”¨è€…ç¢ºå¯¦æœ‰**ç©©å®šçš„é“å¾·ç«‹å ´**\n")
            interpretation.append("- ä¸å¤ªå¯èƒ½æ˜¯æ¸¬é‡å‡è±¡\n")
        elif three_plus < 0.70:
            interpretation.append("### âš ï¸ æ”¯æŒã€Œæ¸¬é‡å‡è±¡ã€å‡è¨­\n")
            interpretation.append(f"- å®Œæˆ3æ¬¡ä»¥ä¸Šå ´æ™¯çš„æ¥µç«¯æ¯”ä¾‹é™è‡³ {three_plus:.1%}\n")
            interpretation.append("- è¡¨ç¤º**è§€æ¸¬æ¬¡æ•¸ä¸è¶³**å°è‡´æ¥µç«¯åˆ†æ•¸\n")
            interpretation.append("- éœ€è¦æ›´å¤šå ´æ™¯æ‰èƒ½æº–ç¢ºæ¸¬é‡é“å¾·å‚¾å‘\n")
        else:
            interpretation.append("### ğŸ¤” æ··åˆæ¨¡å¼\n")
            interpretation.append(f"- å®Œæˆ3æ¬¡ä»¥ä¸Šå ´æ™¯çš„æ¥µç«¯æ¯”ä¾‹ç‚º {three_plus:.1%}\n")
            interpretation.append("- éƒ¨åˆ†ä½¿ç”¨è€…ç¢ºå¯¦æœ‰ç©©å®šç«‹å ´\n")
            interpretation.append("- ä½†æ¸¬é‡é™åˆ¶ä»ç„¶å­˜åœ¨\n")
        
        return "\n".join(interpretation)

if __name__ == '__main__':
    print("æ½›åœ¨é¡åˆ¥åˆ†ææ¨¡çµ„è¼‰å…¥æˆåŠŸ")