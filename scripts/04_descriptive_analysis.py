"""
04_descriptive_analysis.py
==========================
ç¬¬å››æ­¥ï¼šæè¿°æ€§åˆ†æï¼ˆç¬¬3ç« ï¼‰

åŠŸèƒ½ï¼š
1. å…¨çƒé“å¾·åœ°åœ–ï¼ˆ3.1ï¼‰
2. å°ç£èˆ‡æ±äºå®šä½ï¼ˆ3.2ï¼‰
3. éšå±¤å¼åˆ†ç¾¤ï¼ˆ3.3ï¼‰
4. å¢åŠ 3.3è£œå……ï¼šæ›¿ä»£åˆ†ç¾¤æ–¹æ³•æ¯”è¼ƒ
5. æ½›åœ¨é¡åˆ¥åˆ†æï¼ˆ3.4ï¼‰
6. å¢åŠ 3.4è£œå……ï¼šæ•æ„Ÿåº¦åˆ†æ

åŸ·è¡Œæ–¹å¼ï¼š
    python scripts/04_descriptive_analysis.py
"""

import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥è·¯å¾‘
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import logging
from datetime import datetime


def setup_file_logger(log_dir: str = 'outputs/logs') -> logging.Logger:
    """è¨­å®šæª”æ¡ˆæ—¥èªŒè¨˜éŒ„å™¨"""
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    log_file = log_path / 'descriptive_analysis.log'
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    
    logger = logging.getLogger()
    logger.addHandler(file_handler)
    logger.setLevel(logging.INFO)
    
    return logger


def generate_markdown_report(results: dict, output_dir: str = 'report/drafts'):
    """ç”ŸæˆMarkdownæ ¼å¼çš„å ±å‘Šè‰ç¨¿"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    report_file = output_path / 'chapter3_exploration.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç¬¬3ç«  æ¢ç´¢æ€§åˆ†æèˆ‡é¡å‹å­¸å»ºæ§‹\n\n")
        f.write(f"**åˆ†ææ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # 3.1 å…¨çƒé“å¾·åœ°åœ–
        f.write("## 3.1 å…¨çƒé“å¾·åœ°åœ–\n\n")
        f.write("### ç ”ç©¶å•é¡Œ\n\n")
        f.write("- å…¨çƒåœ¨ã€Œå®ˆæ³•vs.æ•ˆç›Šã€è¡çªçš„åˆ†ä½ˆæ¨¡å¼ç‚ºä½•ï¼Ÿ\n")
        f.write("- æ˜¯å¦å‘ˆç¾åœ°ç†èšé›†æˆ–æ–‡åŒ–èšé›†ï¼Ÿ\n\n")
        
        f.write("### è¦–è¦ºåŒ–çµæœ\n\n")
        if 'global' in results:
            f.write(f"- [ä¸–ç•Œåœ°åœ–]({results['global']['world_map']})\n")
            f.write(f"- [æ–‡åŒ–åœˆæ¯”è¼ƒ]({results['global']['cluster_comparison']})\n")
            f.write(f"- [æè¿°çµ±è¨ˆè¡¨]({results['global']['descriptive_stats']})\n\n")
        
        f.write("### é—œéµç™¼ç¾\n\n")
        f.write("ï¼ˆå¾…å¡«å…¥ï¼šæ ¹æ“šåœ–è¡¨çµæœæ’°å¯«ï¼‰\n\n")
        
        # 3.2 å°ç£èˆ‡æ±äºå®šä½
        f.write("## 3.2 å°ç£èˆ‡æ±äºçš„é“å¾·å®šä½\n\n")
        f.write("### ç ”ç©¶å•é¡Œ\n\n")
        f.write("- å°ç£åœ¨9å€‹é“å¾·ç¶­åº¦çš„è¡¨ç¾ç‚ºä½•ï¼Ÿ\n")
        f.write("- èˆ‡æ—¥æœ¬ã€éŸ“åœ‹ã€ä¸­åœ‹å¤§é™¸çš„ç•°åŒï¼Ÿ\n\n")
        
        f.write("### è¦–è¦ºåŒ–çµæœ\n\n")
        if 'east_asia' in results:
            f.write(f"- [æ±äºå››åœ‹é›·é”åœ–]({results['east_asia']['radar_chart']})\n")
            f.write(f"- [è·é›¢ç†±åœ–]({results['east_asia']['distance_heatmap']})\n")
            f.write(f"- [æ¯”è¼ƒè¡¨]({results['east_asia']['comparison_table']})\n\n")
        
        f.write("### é—œéµç™¼ç¾\n\n")
        f.write("ï¼ˆå¾…å¡«å…¥ï¼šæ ¹æ“šåœ–è¡¨çµæœæ’°å¯«ï¼‰\n\n")
        
        # 3.3 éšå±¤å¼åˆ†ç¾¤
        f.write("## 3.3 éšå±¤å¼åˆ†ç¾¤ï¼šé“å¾·è·é›¢çš„æ‹“æ’²çµæ§‹\n\n")
        f.write("### ç ”ç©¶å•é¡Œ\n\n")
        f.write("- åŸºæ–¼é“å¾·åˆ¤æ–·ï¼Œåœ‹å®¶å¦‚ä½•è‡ªç„¶åˆ†ç¾¤ï¼Ÿ\n")
        f.write("- æ˜¯å¦å­˜åœ¨è¶…è¶Šåœ°ç†çš„ã€Œé“å¾·è¦ªç·£é—œä¿‚ã€ï¼Ÿ\n\n")
        
        f.write("### è¦–è¦ºåŒ–çµæœ\n\n")
        if 'hierarchical' in results:
            f.write(f"- [130åœ‹æ¨¹ç‹€åœ–]({results['hierarchical']['dendrogram']})\n")
            f.write(f"- [é“å¾·è·é›¢ç†±åœ–]({results['hierarchical']['distance_heatmap']})\n\n")
            
            f.write("### è©•ä¼°æŒ‡æ¨™\n\n")
            f.write(f"- **Cophenetic Correlation**: {results['hierarchical']['cophenetic_correlation']:.4f}\n")
            f.write(f"- **Adjusted Rand Index**: {results['hierarchical']['ari']:.4f}\n\n")
        
        # 3.3è£œå……
        if 'alternative_clustering' in results:
            f.write("### 3.3è£œå……ï¼šæ›¿ä»£åˆ†ç¾¤æ–¹æ³•æ¯”è¼ƒ\n\n")
            f.write(f"- [K-meansè©•ä¼°]({results['alternative_clustering']['plots']['kmeans_evaluation']})\n")
            f.write(f"- [t-SNEè¦–è¦ºåŒ–]({results['alternative_clustering']['plots']['tsne_visualization']})\n")
            f.write(f"- [æ–¹æ³•æ¯”è¼ƒ]({results['alternative_clustering']['plots']['methods_comparison']})\n\n")
        
        f.write("### é—œéµç™¼ç¾\n\n")
        f.write("ï¼ˆå¾…å¡«å…¥ï¼šæ ¹æ“šåœ–è¡¨çµæœæ’°å¯«ï¼‰\n\n")
        
        # 3.4 æ½›åœ¨é¡åˆ¥åˆ†æ
        f.write("## 3.4 æ½›åœ¨é¡åˆ¥åˆ†æï¼šé“å¾·äººæ ¼é¡å‹å­¸\n\n")
        f.write("### ç ”ç©¶å•é¡Œ\n\n")
        f.write("- æ˜¯å¦å­˜åœ¨ä¸åŒçš„ã€Œé“å¾·æ±ºç­–æ¨¡å¼ã€ï¼Ÿ\n")
        f.write("- é€™äº›æ¨¡å¼æ˜¯å¦å°æ‡‰å€«ç†ç†è«–ï¼Ÿ\n\n")
        
        f.write("### è¦–è¦ºåŒ–çµæœ\n\n")
        if 'lca' in results:
            f.write(f"- [BICæ›²ç·š]({results['lca']['bic_curve']})\n")
            f.write(f"- [é¡åˆ¥é›·é”åœ–]({results['lca']['class_radar']})\n")
            f.write(f"- [æ–‡åŒ–åˆ†ä½ˆ]({results['lca']['culture_distribution']})\n\n")
            
            f.write(f"### æœ€ä½³é¡åˆ¥æ•¸: {results['lca']['optimal_k']}\n\n")
        
        # 3.4è£œå……
        if 'lca_sensitivity' in results:
            f.write("### 3.4è£œå……ï¼šæ•æ„Ÿåº¦åˆ†æ\n\n")
            f.write(f"- [æ¥µç«¯æ¯”ä¾‹æ¯”è¼ƒ]({results['lca_sensitivity']['extreme_plot']})\n")
            f.write(f"- [è©®é‡‹å ±å‘Š](outputs/tables/chapter3/lca_sensitivity_interpretation.md)\n\n")
        
        f.write("### é—œéµç™¼ç¾\n\n")
        f.write("ï¼ˆå¾…å¡«å…¥ï¼šæ ¹æ“šåœ–è¡¨çµæœæ’°å¯«ï¼‰\n\n")
        
        f.write("---\n\n")
        f.write("**è¨»**: æœ¬å ±å‘Šç‚ºè‡ªå‹•ç”Ÿæˆçš„è‰ç¨¿ï¼Œé—œéµç™¼ç¾éœ€æ ¹æ“šåœ–è¡¨çµæœæ‰‹å‹•å¡«å¯«ã€‚\n")
    
    print(f"âœ… Markdownå ±å‘Š: {report_file}")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "=" * 60)
    print("ğŸ“Š MIT Moral Machine - æ¢ç´¢æ€§åˆ†æ (Step 04)")
    print("=" * 60)
    
    # è¨­å®šæ—¥èªŒ
    logger = setup_file_logger()
    logger.info("é–‹å§‹åŸ·è¡Œæ¢ç´¢æ€§åˆ†æè…³æœ¬...")
    
    try:
        # è¼‰å…¥è³‡æ–™
        print("\nã€è¼‰å…¥è³‡æ–™ã€‘")
        featured_file = Path('data/processed/featured_data.csv')
        
        if not featured_file.exists():
            print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {featured_file}")
            print("è«‹å…ˆåŸ·è¡Œ 03_feature_engineering.py")
            return
        
        df = pd.read_csv(featured_file)
        print(f"âœ… è¼‰å…¥è³‡æ–™: {len(df):,} è¡Œ")
        
        # ç¯©é¸è³‡æ–™ï¼šæ’é™¤ Cluster == -1
        if 'has_country_features' not in df.columns:
            print("âš ï¸  æ‰¾ä¸åˆ° has_country_features æ¬„ä½ï¼Œä½¿ç”¨ Cluster != -1 ç¯©é¸")
            df_ch3 = df[df['Cluster'] != -1].copy()
        else:
            df_ch3 = df[df['has_country_features']].copy()
        
        print(f"âœ… ç¬¬3ç« åˆ†æè³‡æ–™: {len(df_ch3):,} è¡Œ")
        print(f"   å·²æ’é™¤ {len(df) - len(df_ch3):,} è¡Œ (Cluster == -1)")
        
        results = {}
        
        # ============================================
        # 3.1 å…¨çƒé“å¾·åœ°åœ–
        # ============================================
        print("\n" + "=" * 60)
        print("ã€3.1ã€‘å…¨çƒé“å¾·åœ°åœ–")
        print("=" * 60)
        
        from src.analysis.descriptive.global_patterns import GlobalPatternAnalyzer
        
        global_analyzer = GlobalPatternAnalyzer()
        results['global'] = global_analyzer.run_analysis(df_ch3)
        
        # ============================================
        # 3.2 å°ç£èˆ‡æ±äºå®šä½
        # ============================================
        print("\n" + "=" * 60)
        print("ã€3.2ã€‘å°ç£èˆ‡æ±äºå®šä½")
        print("=" * 60)
        
        from src.analysis.descriptive.east_asia_focus import EastAsiaAnalyzer
        
        east_asia_analyzer = EastAsiaAnalyzer()
        results['east_asia'] = east_asia_analyzer.run_analysis()
        
        # ============================================
        # 3.3 éšå±¤å¼åˆ†ç¾¤
        # ============================================
        print("\n" + "=" * 60)
        print("ã€3.3ã€‘éšå±¤å¼åˆ†ç¾¤")
        print("=" * 60)
        
        from src.analysis.clustering.hierarchical import HierarchicalClusterAnalyzer
        
        hierarchical_analyzer = HierarchicalClusterAnalyzer()
        results['hierarchical'] = hierarchical_analyzer.run_analysis()
        
        # ============================================
        # 3.4 æ½›åœ¨é¡åˆ¥åˆ†æ
        # ============================================
        print("\n" + "=" * 60)
        print("ã€3.4ã€‘æ½›åœ¨é¡åˆ¥åˆ†æ")
        print("=" * 60)
        
        from src.analysis.clustering.latent_class import LatentClassAnalyzer
        
        lca_analyzer = LatentClassAnalyzer()
        results['lca'] = lca_analyzer.run_analysis(df_ch3)
        
        # ============================================
        # ã€æ–°å¢ã€‘3.3è£œå……ï¼šæ›¿ä»£åˆ†ç¾¤æ–¹æ³•æ¯”è¼ƒ
        # ============================================
        RUN_ALTERNATIVE_CLUSTERING = True  # è¨­ç‚ºFalseå¯è·³é
        
        if RUN_ALTERNATIVE_CLUSTERING:
            print("\n" + "=" * 60)
            print("ã€3.3è£œå……ã€‘æ›¿ä»£åˆ†ç¾¤æ–¹æ³•æ¯”è¼ƒ")
            print("=" * 60)
            
            try:
                from src.analysis.clustering.alternative_methods import AlternativeClusteringAnalyzer
                
                alt_analyzer = AlternativeClusteringAnalyzer()
                results['alternative_clustering'] = alt_analyzer.run_full_analysis()
            except Exception as e:
                logger.warning(f"æ›¿ä»£åˆ†ç¾¤æ–¹æ³•åŸ·è¡Œå¤±æ•—: {e}")
                print(f"âš ï¸  æ›¿ä»£åˆ†ç¾¤æ–¹æ³•è·³é: {e}")
        
        # ============================================
        # ã€æ–°å¢ã€‘3.4è£œå……ï¼šæ•æ„Ÿåº¦åˆ†æ
        # ============================================
        RUN_SENSITIVITY_ANALYSIS = True  # è¨­ç‚ºFalseå¯è·³é
        
        if RUN_SENSITIVITY_ANALYSIS:
            print("\n" + "=" * 60)
            print("ã€3.4è£œå……ã€‘LCAæ•æ„Ÿåº¦åˆ†æ")
            print("=" * 60)
            
            try:
                results['lca_sensitivity'] = lca_analyzer.run_sensitivity_analysis()
            except Exception as e:
                logger.warning(f"æ•æ„Ÿåº¦åˆ†æåŸ·è¡Œå¤±æ•—: {e}")
                print(f"âš ï¸  æ•æ„Ÿåº¦åˆ†æè·³é: {e}")
        
        # ============================================
        # ç”Ÿæˆå ±å‘Š
        # ============================================
        print("\n" + "=" * 60)
        print("ã€ç”Ÿæˆå ±å‘Šã€‘")
        print("=" * 60)
        
        generate_markdown_report(results)
        
        # å®Œæˆ
        print("\n" + "=" * 60)
        print("âœ… æ¢ç´¢æ€§åˆ†æå®Œæˆï¼")
        print("=" * 60)
        print("\nğŸ“Š å·²ç”¢ç”Ÿä»¥ä¸‹è¼¸å‡º:")
        
        print("\nã€3.1 å…¨çƒé“å¾·åœ°åœ–ã€‘")
        for key, value in results['global'].items():
            print(f"  - {key}: {value}")
        
        print("\nã€3.2 å°ç£èˆ‡æ±äºå®šä½ã€‘")
        for key, value in results['east_asia'].items():
            print(f"  - {key}: {value}")
        
        print("\nã€3.3 éšå±¤å¼åˆ†ç¾¤ã€‘")
        print(f"  - dendrogram: {results['hierarchical']['dendrogram']}")
        print(f"  - distance_heatmap: {results['hierarchical']['distance_heatmap']}")
        print(f"  - Cophenetic Correlation: {results['hierarchical']['cophenetic_correlation']:.4f}")
        print(f"  - ARI: {results['hierarchical']['ari']:.4f}")
        
        # ã€æ–°å¢ã€‘é¡¯ç¤ºè£œå……åˆ†æçµæœ
        if 'alternative_clustering' in results:
            print("\nã€3.3è£œå…… æ›¿ä»£åˆ†ç¾¤æ–¹æ³•ã€‘")
            print(f"  - K-meansæœ€ä½³k: {results['alternative_clustering']['kmeans']['best_k']}")
            print(f"  - æ–¹æ³•æ¯”è¼ƒè¡¨: outputs/tables/chapter3/clustering_methods_comparison.csv")
            print(f"  - t-SNEè¦–è¦ºåŒ–: {results['alternative_clustering']['plots']['tsne_visualization']}")
        
        print("\nã€3.4 æ½›åœ¨é¡åˆ¥åˆ†æã€‘")
        print(f"  - bic_curve: {results['lca']['bic_curve']}")
        print(f"  - class_radar: {results['lca']['class_radar']}")
        print(f"  - culture_distribution: {results['lca']['culture_distribution']}")
        print(f"  - æœ€ä½³é¡åˆ¥æ•¸: {results['lca']['optimal_k']}")
        
        # ã€æ–°å¢ã€‘é¡¯ç¤ºæ•æ„Ÿåº¦åˆ†æçµæœ
        if 'lca_sensitivity' in results:
            print("\nã€3.4è£œå…… æ•æ„Ÿåº¦åˆ†æã€‘")
            print(f"  - æ¥µç«¯æ¯”ä¾‹åœ–: {results['lca_sensitivity']['extreme_plot']}")
            print(f"  - çµ±è¨ˆæ‘˜è¦: outputs/tables/chapter3/lca_sensitivity_summary.csv")
            print(f"  - è©®é‡‹å ±å‘Š: outputs/tables/chapter3/lca_sensitivity_interpretation.md")
        
        print("\nğŸ“„ å ±å‘Šè‰ç¨¿:")
        print("  - report/drafts/chapter3_exploration.md")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥: æ ¹æ“šåœ–è¡¨çµæœå¡«å¯«å ±å‘Šä¸­çš„ã€Œé—œéµç™¼ç¾ã€")
        print("=" * 60 + "\n")
        
        logger.info("æ¢ç´¢æ€§åˆ†æè…³æœ¬åŸ·è¡Œå®Œæˆ")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        print(f"\nâŒ éŒ¯èª¤: {e}")
        raise


if __name__ == '__main__':
    main()