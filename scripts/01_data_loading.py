"""
01_data_loading.py
==================
ç¬¬ä¸€æ­¥ï¼šè³‡æ–™è¼‰å…¥

åŠŸèƒ½ï¼š
1. è¼‰å…¥æ‰€æœ‰åŸå§‹è³‡æ–™æª”æ¡ˆ
2. é€²è¡ŒåŸºæœ¬é©—è­‰èˆ‡å®Œæ•´æ€§æª¢æŸ¥
3. ç”¢ç”Ÿè³‡æ–™å“è³ªå ±å‘Š
4. å„²å­˜è¼‰å…¥æ—¥èªŒ

åŸ·è¡Œæ–¹å¼ï¼š
    python scripts/01_data_loading.py
"""

import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥è·¯å¾‘
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.data.loader import DataLoader
import pandas as pd
import logging
from datetime import datetime
import json

def setup_file_logger(log_dir: str = 'outputs/logs') -> logging.Logger:
    """
    è¨­å®šæª”æ¡ˆæ—¥èªŒè¨˜éŒ„å™¨
    
    Parameters:
    -----------
    log_dir : str
        æ—¥èªŒç›®éŒ„è·¯å¾‘
    """
    # å»ºç«‹æ—¥èªŒç›®éŒ„
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    # å»ºç«‹æª”æ¡ˆhandler
    log_file = log_path / 'data_loading.log'
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # æ ¼å¼è¨­å®š
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    
    # å–å¾—root loggerä¸¦æ·»åŠ handler
    logger = logging.getLogger()
    logger.addHandler(file_handler)
    logger.setLevel(logging.INFO)
    
    return logger

def generate_data_quality_report(data: dict, output_dir: str = 'outputs/tables/chapter2'):
    """
    ç”Ÿæˆè³‡æ–™å“è³ªå ±å‘Š
    
    Parameters:
    -----------
    data : dict
        è¼‰å…¥çš„è³‡æ–™å­—å…¸
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "=" * 60)
    print("ç”Ÿæˆè³‡æ–™å“è³ªå ±å‘Š...")
    print("=" * 60)
    
    # 1. æ•´é«”æ‘˜è¦
    summary_data = []
    
    for name, df in data.items():
        summary_data.append({
            'è³‡æ–™é›†': name,
            'åˆ—æ•¸': f"{len(df):,}",
            'æ¬„æ•¸': len(df.columns),
            'è¨˜æ†¶é«”(MB)': f"{df.memory_usage(deep=True).sum() / 1024**2:.2f}",
            'ç¸½ç¼ºå¤±å€¼': f"{df.isnull().sum().sum():,}",
            'ç¼ºå¤±å€¼æ¯”ä¾‹': f"{df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100:.2f}%"
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_file = output_path / 'data_loading_summary.csv'
    summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
    print(f"âœ… æ•´é«”æ‘˜è¦: {summary_file}")
    
    # 2. å„è³‡æ–™é›†çš„æ¬„ä½è³‡è¨Š
    for name, df in data.items():
        col_info = []
        
        for col in df.columns:
            col_info.append({
                'æ¬„ä½åç¨±': col,
                'è³‡æ–™å‹æ…‹': str(df[col].dtype),
                'éç©ºå€¼æ•¸é‡': f"{df[col].count():,}",
                'ç¼ºå¤±å€¼æ•¸é‡': f"{df[col].isnull().sum():,}",
                'ç¼ºå¤±å€¼æ¯”ä¾‹': f"{df[col].isnull().sum() / len(df) * 100:.2f}%",
                'å”¯ä¸€å€¼æ•¸é‡': f"{df[col].nunique():,}",
            })
        
        col_df = pd.DataFrame(col_info)
        col_file = output_path / f'{name}_columns_info.csv'
        col_df.to_csv(col_file, index=False, encoding='utf-8-sig')
        print(f"âœ… {name} æ¬„ä½è³‡è¨Š: {col_file}")
    
    # 3. å•å·è³‡æ–™çš„å ´æ™¯é¡å‹åˆ†ä½ˆ
    if 'survey' in data:
        survey_df = data['survey']
        
        if 'ScenarioType' in survey_df.columns:
            scenario_dist = survey_df['ScenarioType'].value_counts().reset_index()
            scenario_dist.columns = ['å ´æ™¯é¡å‹', 'æ•¸é‡']
            scenario_dist['æ¯”ä¾‹'] = (scenario_dist['æ•¸é‡'] / len(survey_df) * 100).round(2).astype(str) + '%'
            
            scenario_file = output_path / 'scenario_type_distribution.csv'
            scenario_dist.to_csv(scenario_file, index=False, encoding='utf-8-sig')
            print(f"âœ… å ´æ™¯é¡å‹åˆ†ä½ˆ: {scenario_file}")
        
        # 4. åœ‹å®¶åˆ†ä½ˆ
        if 'UserCountry3' in survey_df.columns:
            country_dist = survey_df['UserCountry3'].value_counts().head(20).reset_index()
            country_dist.columns = ['åœ‹å®¶ä»£ç¢¼', 'æ±ºç­–æ•¸é‡']
            
            country_file = output_path / 'top20_countries.csv'
            country_dist.to_csv(country_file, index=False, encoding='utf-8-sig')
            print(f"âœ… å‰20ååœ‹å®¶: {country_file}")
    
    print("=" * 60)

def generate_markdown_report(data: dict, summary: dict, output_dir: str = 'report/drafts'):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„å ±å‘Šè‰ç¨¿
    
    Parameters:
    -----------
    data : dict
        è¼‰å…¥çš„è³‡æ–™å­—å…¸
    summary : dict
        è¼‰å…¥æ‘˜è¦
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    report_file = output_path / 'chapter2_section1_data_loading.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç¬¬2ç«  è³‡æ–™è™•ç†\n\n")
        f.write("## 2.1 è³‡æ–™ä¾†æºèˆ‡è¼‰å…¥\n\n")
        f.write(f"**è¼‰å…¥æ™‚é–“**: {summary['loading_time']}\n\n")
        
        f.write("### è³‡æ–™é›†æ¦‚è¦½\n\n")
        f.write("| è³‡æ–™é›† | åˆ—æ•¸ | æ¬„æ•¸ | è¨˜æ†¶é«”(MB) | ç¼ºå¤±å€¼æ¯”ä¾‹ |\n")
        f.write("|--------|------|------|-----------|------------|\n")
        
        for name, info in summary['datasets'].items():
            f.write(f"| {name} | {info['rows']:,} | {info['columns']} | "
                   f"{info['memory_mb']:.2f} | {info['missing_pct']} |\n")
        
        f.write("\n### å„è³‡æ–™é›†èªªæ˜\n\n")
        
        # SharedResponsesSurvey
        if 'survey' in data:
            df = data['survey']
            f.write("#### 1. SharedResponsesSurvey.csv\n\n")
            f.write(f"- **ç”¨é€”**: ä¸»è¦åˆ†æè³‡æ–™ï¼ŒåŒ…å«å ´æ™¯å›æ‡‰èˆ‡äººå£çµ±è¨ˆè®Šæ•¸\n")
            f.write(f"- **ç¶­åº¦**: {len(df):,} è¡Œ Ã— {len(df.columns)} æ¬„\n")
            f.write(f"- **é—œéµæ¬„ä½**: ResponseID, UserID, UserCountry3, Saved, ScenarioType\n\n")
            
            if 'ScenarioType' in df.columns:
                f.write("**å ´æ™¯é¡å‹åˆ†ä½ˆ**:\n\n")
                scenario_counts = df['ScenarioType'].value_counts()
                for stype, count in scenario_counts.items():
                    pct = count / len(df) * 100
                    f.write(f"- {stype}: {count:,} ({pct:.1f}%)\n")
                f.write("\n")
        
        # CountriesChangePr
        if 'countries_change' in data:
            df = data['countries_change']
            f.write("#### 2. CountriesChangePr.csv\n\n")
            f.write(f"- **ç”¨é€”**: åœ‹å®¶å±¤ç´šçµ±è¨ˆï¼ŒåŒ…å«9å€‹é“å¾·å±¬æ€§çš„AMCEå€¼\n")
            f.write(f"- **ç¶­åº¦**: {len(df)} å€‹åœ‹å®¶ Ã— {len(df.columns)} å€‹æŒ‡æ¨™\n")
            f.write(f"- **åŒ…å«**: 9å° (Estimates + se) æ¬„ä½\n\n")
        
        # cluster_map
        if 'cluster_map' in data:
            df = data['cluster_map']
            f.write("#### 3. country_cluster_map.csv\n\n")
            f.write(f"- **ç”¨é€”**: æ–‡åŒ–åœˆåˆ†é¡\n")
            f.write(f"- **ç¶­åº¦**: {len(df)} å€‹åœ‹å®¶\n\n")
            
            cluster_counts = df['Cluster'].value_counts().sort_index()
            f.write("**æ–‡åŒ–åœˆåˆ†ä½ˆ**:\n\n")
            cluster_names = {0: 'Western', 1: 'Eastern', 2: 'Southern'}
            for cluster, count in cluster_counts.items():
                cluster_name = cluster_names.get(cluster, f'Cluster {cluster}')
                f.write(f"- {cluster_name} (Cluster {cluster}): {count} å€‹åœ‹å®¶\n")
            f.write("\n")
        
        # moral_distance
        if 'moral_distance' in data:
            df = data['moral_distance']
            f.write("#### 4. moral_distance.csv\n\n")
            f.write(f"- **ç”¨é€”**: åœ‹å®¶é–“é“å¾·è·é›¢çŸ©é™£\n")
            f.write(f"- **åŸºæº–åœ‹**: ç¾åœ‹ (Distance = 0)\n")
            f.write(f"- **è·é›¢ç¯„åœ**: {df['Distance'].min():.3f} ~ {df['Distance'].max():.3f}\n")
            f.write(f"- **å¹³å‡è·é›¢**: {df['Distance'].mean():.3f}\n\n")
        
        # dendrogram
        if 'dendrogram' in data:
            df = data['dendrogram']
            countries = df[df['culture'].notna()]
            f.write("#### 5. dendrogram_Culture.csv\n\n")
            f.write(f"- **ç”¨é€”**: éšå±¤åˆ†ç¾¤æ¨¹ç‹€åœ–è³‡æ–™\n")
            f.write(f"- **ç¯€é»ç¸½æ•¸**: {len(df)}\n")
            f.write(f"- **åœ‹å®¶ç¯€é»**: {len(countries)}\n\n")
        
        f.write("### è³‡æ–™å“è³ªè©•ä¼°\n\n")
        f.write("æ‰€æœ‰è³‡æ–™æª”æ¡ˆå·²æˆåŠŸè¼‰å…¥ï¼Œé—œéµæ¬„ä½å®Œæ•´ç„¡ç¼ºå¤±ã€‚")
        f.write("å¾ŒçºŒå°‡é€²è¡Œè³‡æ–™æ¸…ç†èˆ‡è½‰æ›ã€‚\n\n")
    
    print(f"\nâœ… Markdownå ±å‘Š: {report_file}")

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "=" * 60)
    print("ğŸ“‚ MIT Moral Machine - è³‡æ–™è¼‰å…¥ (Step 01)")
    print("=" * 60)
    
    # è¨­å®šæª”æ¡ˆæ—¥èªŒ
    logger = setup_file_logger()
    logger.info("é–‹å§‹åŸ·è¡Œè³‡æ–™è¼‰å…¥è…³æœ¬...")
    
    try:
        # åˆå§‹åŒ–è¼‰å…¥å™¨
        loader = DataLoader(data_dir='data/raw')
        
        # æª¢æŸ¥æª”æ¡ˆ
        files_status = loader.check_files_exist()
        
        if not all(files_status.values()):
            missing_files = [k for k, v in files_status.items() if not v]
            logger.error(f"ç¼ºå°‘ä»¥ä¸‹æª”æ¡ˆ: {missing_files}")
            print(f"\nâŒ éŒ¯èª¤: ç¼ºå°‘è³‡æ–™æª”æ¡ˆ {missing_files}")
            print("è«‹ç¢ºèª data/raw/ ç›®éŒ„ä¸‹æœ‰æ‰€æœ‰å¿…è¦æª”æ¡ˆ")
            return
        
        # è¼‰å…¥æ‰€æœ‰è³‡æ–™
        print("\nã€é–‹å§‹è¼‰å…¥è³‡æ–™ã€‘")
        data = loader.load_all_data(survey_nrows=None)  # None = è¼‰å…¥å…¨éƒ¨
        
        # åˆ—å°æ‘˜è¦
        loader.print_summary()
        
        # ç”Ÿæˆè¼‰å…¥æ‘˜è¦
        summary = loader.generate_loading_summary()
        
        # å„²å­˜æ‘˜è¦ç‚ºJSON
        json_path = Path('outputs/tables/chapter2')
        json_path.mkdir(parents=True, exist_ok=True)
        with open(json_path / 'loading_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆè³‡æ–™å“è³ªå ±å‘Š
        generate_data_quality_report(data)
        
        # ç”ŸæˆMarkdownå ±å‘Šè‰ç¨¿
        generate_markdown_report(data, summary)
        
        print("\n" + "=" * 60)
        print("âœ… è³‡æ–™è¼‰å…¥å®Œæˆï¼")
        print("=" * 60)
        print("\nğŸ“Š å·²ç”¢ç”Ÿä»¥ä¸‹è¼¸å‡º:")
        print("  - outputs/logs/data_loading.log")
        print("  - outputs/tables/chapter2/data_loading_summary.csv")
        print("  - outputs/tables/chapter2/*_columns_info.csv")
        print("  - report/drafts/chapter2_section1_data_loading.md")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥: python scripts/02_data_cleaning.py")
        print("=" * 60 + "\n")
        
        logger.info("è³‡æ–™è¼‰å…¥è…³æœ¬åŸ·è¡Œå®Œæˆ")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        print(f"\nâŒ éŒ¯èª¤: {e}")
        raise

if __name__ == '__main__':
    main()