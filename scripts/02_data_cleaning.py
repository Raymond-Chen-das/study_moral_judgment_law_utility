"""
02_data_cleaning.py
===================
ç¬¬äºŒæ­¥ï¼šè³‡æ–™æ¸…ç†

åŠŸèƒ½ï¼š
1. è™•ç†ç¼ºå¤±å€¼
2. è™•ç†ç•°å¸¸å€¼
3. ç¯©é¸ã€Œå®ˆæ³•vs.æ•ˆç›Šã€è¡çªæƒ…å¢ƒ
4. æª¢æŸ¥å ´æ™¯å®Œæ•´æ€§
5. åˆä½µæ–‡åŒ–åœˆåˆ†é¡
6. ç”¢ç”Ÿæ¸…ç†å ±å‘Š

åŸ·è¡Œæ–¹å¼ï¼š
    python scripts/02_data_cleaning.py
"""

import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥è·¯å¾‘
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.data.loader import DataLoader
from src.data.cleaner import DataCleaner
import pandas as pd
import logging
from datetime import datetime
import json

def setup_file_logger(log_dir: str = 'outputs/logs') -> logging.Logger:
    """
    è¨­å®šæª”æ¡ˆæ—¥èªŒè¨˜éŒ„å™¨
    
    Parameters:
    -----------
    log_dir : str
        æ—¥èªŒç›®éŒ„è·¯å¾‘
    """
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    log_file = log_path / 'data_cleaning.log'
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    
    logger = logging.getLogger()
    logger.addHandler(file_handler)
    logger.setLevel(logging.INFO)
    
    return logger

def save_cleaned_data(df: pd.DataFrame, output_dir: str = 'data/processed'):
    """
    å„²å­˜æ¸…ç†å¾Œçš„è³‡æ–™
    
    Parameters:
    -----------
    df : pd.DataFrame
        æ¸…ç†å¾Œçš„è³‡æ–™æ¡†
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    output_file = output_path / 'cleaned_survey.csv'
    
    print(f"\nå„²å­˜æ¸…ç†å¾Œçš„è³‡æ–™...")
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    file_size_mb = output_file.stat().st_size / 1024**2
    print(f"âœ… å·²å„²å­˜: {output_file}")
    print(f"   æª”æ¡ˆå¤§å°: {file_size_mb:.2f} MB")

def generate_cleaning_report(report: dict, output_dir: str = 'outputs/tables/chapter2'):
    """
    ç”Ÿæˆæ¸…ç†å ±å‘ŠCSV
    
    Parameters:
    -----------
    report : dict
        æ¸…ç†å ±å‘Šå­—å…¸
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("\nç”Ÿæˆæ¸…ç†å ±å‘Š...")
    
    # 1. å„æ­¥é©Ÿçµ±è¨ˆè¡¨
    steps_data = []
    original = report['original_rows']
    
    steps_data.append({
        'æ­¥é©Ÿ': '0. åŸå§‹è³‡æ–™',
        'è¡Œæ•¸': f"{original:,}",
        'åˆªé™¤/éæ¿¾': '-',
        'ä¿ç•™æ¯”ä¾‹': '100.00%'
    })
    
    for i, step in enumerate(report['steps'], 1):
        step_names = {
            'remove_missing_key_vars': f'{i}. åˆªé™¤é—œéµè®Šæ•¸ç¼ºå¤±',
            'remove_outliers': f'{i}. åˆªé™¤ç•°å¸¸å€¼',
            'filter_law_vs_utility': f'{i}. ç¯©é¸å®ˆæ³•vs.æ•ˆç›Šæƒ…å¢ƒ',
            'check_completeness': f'{i}. æª¢æŸ¥å ´æ™¯å®Œæ•´æ€§',
            'merge_cluster': f'{i}. åˆä½µæ–‡åŒ–åœˆåˆ†é¡'
        }
        
        step_name = step_names.get(step['step'], step['step'])
        remaining = step['remaining']
        removed = step.get('removed', 0)
        retention = (remaining / original * 100) if original > 0 else 0
        
        steps_data.append({
            'æ­¥é©Ÿ': step_name,
            'è¡Œæ•¸': f"{remaining:,}",
            'åˆªé™¤/éæ¿¾': f"{removed:,}" if removed > 0 else '-',
            'ä¿ç•™æ¯”ä¾‹': f"{retention:.2f}%"
        })
    
    steps_df = pd.DataFrame(steps_data)
    steps_file = output_path / 'cleaning_steps_summary.csv'
    steps_df.to_csv(steps_file, index=False, encoding='utf-8-sig')
    print(f"âœ… æ¸…ç†æ­¥é©Ÿæ‘˜è¦: {steps_file}")
    
    # 2. å„²å­˜å®Œæ•´å ±å‘Šç‚ºJSON
    json_file = output_path / 'cleaning_report.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"âœ… å®Œæ•´å ±å‘Š(JSON): {json_file}")

def generate_markdown_report(report: dict, 
                            cleaned_df: pd.DataFrame,
                            output_dir: str = 'report/drafts'):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„å ±å‘Šè‰ç¨¿
    
    Parameters:
    -----------
    report : dict
        æ¸…ç†å ±å‘Š
    cleaned_df : pd.DataFrame
        æ¸…ç†å¾Œçš„è³‡æ–™
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    report_file = output_path / 'chapter2_section2_data_cleaning.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç¬¬2ç«  è³‡æ–™è™•ç†\n\n")
        f.write("## 2.2 è³‡æ–™æ¸…ç†\n\n")
        
        # æ¸…ç†æµç¨‹
        f.write("### æ¸…ç†æµç¨‹\n\n")
        f.write("æœ¬ç ”ç©¶èšç„¦æ–¼ã€Œå®ˆæ³•vs.æ•ˆç›Šã€çš„é“å¾·å…©é›£æƒ…å¢ƒï¼Œ")
        f.write("å› æ­¤éœ€è¦ç¯©é¸ç¬¦åˆæ­¤æ¢ä»¶çš„å ´æ™¯ï¼Œä¸¦è™•ç†è³‡æ–™å“è³ªå•é¡Œã€‚\n\n")
        
        # æ¸…ç†æ­¥é©Ÿè¡¨æ ¼
        f.write("### æ¸…ç†æ­¥é©Ÿæ‘˜è¦\n\n")
        f.write("| æ­¥é©Ÿ | è¡Œæ•¸ | åˆªé™¤/éæ¿¾ | ä¿ç•™æ¯”ä¾‹ |\n")
        f.write("|------|------|-----------|----------|\n")
        
        original = report['original_rows']
        f.write(f"| åŸå§‹è³‡æ–™ | {original:,} | - | 100.00% |\n")
        
        step_names = {
            'remove_missing_key_vars': 'åˆªé™¤é—œéµè®Šæ•¸ç¼ºå¤±',
            'remove_outliers': 'åˆªé™¤ç•°å¸¸å€¼',
            'filter_law_vs_utility': 'ç¯©é¸å®ˆæ³•vs.æ•ˆç›Šæƒ…å¢ƒ',
            'check_completeness': 'æª¢æŸ¥å ´æ™¯å®Œæ•´æ€§',
            'merge_cluster': 'åˆä½µæ–‡åŒ–åœˆåˆ†é¡'
        }
        
        for step in report['steps']:
            step_name = step_names.get(step['step'], step['step'])
            remaining = step['remaining']
            removed = step.get('removed', 0)
            retention = (remaining / original * 100) if original > 0 else 0
            
            removed_str = f"{removed:,}" if removed > 0 else "-"
            f.write(f"| {step_name} | {remaining:,} | {removed_str} | {retention:.2f}% |\n")
        
        # å„æ­¥é©Ÿè©³ç´°èªªæ˜
        f.write("\n### å„æ­¥é©Ÿèªªæ˜\n\n")
        
        f.write("#### Step 1: åˆªé™¤é—œéµè®Šæ•¸ç¼ºå¤±\n\n")
        f.write("é—œéµè®Šæ•¸åŒ…æ‹¬ï¼š`Saved`ã€`ScenarioType`ã€`UserCountry3`ã€`ResponseID`ã€‚")
        f.write("é€™äº›è®Šæ•¸å°æ–¼åˆ†æè‡³é—œé‡è¦ï¼Œç¼ºå¤±å‰‡ç„¡æ³•é€²è¡Œå¾ŒçºŒåˆ†æã€‚\n\n")
        
        step1 = next((s for s in report['steps'] if s['step'] == 'remove_missing_key_vars'), None)
        if step1:
            f.write(f"- **åˆªé™¤**: {step1['removed']:,} è¡Œ ({step1['removed_pct']})\n")
            f.write(f"- **å‰©é¤˜**: {step1['remaining']:,} è¡Œ\n\n")
        
        f.write("#### Step 2: åˆªé™¤ç•°å¸¸å€¼\n\n")
        f.write("æª¢æŸ¥ä»¥ä¸‹è®Šæ•¸çš„ç•°å¸¸å€¼ï¼š\n")
        f.write("- `Review_age`: å¹´é½¡ç¯„åœ [18, 75]\n")
        f.write("- `Review_political`: æ”¿æ²»ç«‹å ´ [0, 1]\n")
        f.write("- `Review_religious`: å®—æ•™ç¨‹åº¦ [0, 1]\n\n")
        f.write("**è™•ç†ç­–ç•¥**: ç›´æ¥åˆªé™¤è¶…å‡ºåˆç†ç¯„åœçš„è³‡æ–™ï¼Œ")
        f.write("é¿å…å¡«è£œå¯èƒ½å¼•å…¥çš„åèª¤ã€‚\n\n")
        
        step2 = next((s for s in report['steps'] if s['step'] == 'remove_outliers'), None)
        if step2:
            f.write(f"- **åˆªé™¤**: {step2['removed']:,} è¡Œ ({step2['removed_pct']})\n")
            f.write(f"- **å‰©é¤˜**: {step2['remaining']:,} è¡Œ\n\n")
        
        f.write("#### Step 3: ç¯©é¸ã€Œå®ˆæ³•vs.æ•ˆç›Šã€è¡çªæƒ…å¢ƒ\n\n")
        f.write("æœ¬ç ”ç©¶èšç„¦æ–¼é“å¾·å…©é›£æƒ…å¢ƒï¼Œéœ€åŒæ™‚æ»¿è¶³ä»¥ä¸‹æ¢ä»¶ï¼š\n\n")
        f.write("1. **ScenarioType = 'Utilitarian'**: å ´æ™¯æ¶‰åŠäººæ•¸å·®ç•°\n")
        f.write("2. **CrossingSignal âˆˆ {1, 2}**: æœ‰æ³•å¾‹è€ƒé‡ï¼ˆç¶ ç‡ˆåˆæ³•æˆ–ç´…ç‡ˆé•æ³•ï¼‰\n")
        f.write("3. **DiffNumberOFCharacters > 0**: å…©å´äººæ•¸ç¢ºå¯¦æœ‰å·®ç•°\n\n")
        f.write("**ç¯©é¸é‚è¼¯**: åªæœ‰ç•¶ã€Œå®ˆæ³•ã€å’Œã€Œæ•‘å¤šæ•¸ã€ç”¢ç”Ÿè¡çªæ™‚ï¼Œ")
        f.write("æ‰æ§‹æˆçœŸæ­£çš„é“å¾·å…©é›£ã€‚ä¾‹å¦‚ï¼š\n\n")
        f.write("- âœ… **æœ‰è¡çª**: 3äººé—–ç´…ç‡ˆ vs. 1äººç­‰ç¶ ç‡ˆ\n")
        f.write("- âŒ **ç„¡è¡çª**: 5äººç­‰ç¶ ç‡ˆ vs. 3äººé—–ç´…ç‡ˆï¼ˆå®ˆæ³•å’Œæ•‘å¤šæ•¸ä¸€è‡´ï¼‰\n\n")
        
        step3 = next((s for s in report['steps'] if s['step'] == 'filter_law_vs_utility'), None)
        if step3:
            f.write(f"- **ä¿ç•™**: {step3['remaining']:,} è¡Œ ({step3['remaining_pct']})\n")
            f.write(f"- **éæ¿¾**: {step3['removed']:,} è¡Œ\n\n")
        
        f.write("#### Step 4: æª¢æŸ¥å ´æ™¯å®Œæ•´æ€§\n\n")
        f.write("æ¯å€‹å ´æ™¯ï¼ˆ`ResponseID`ï¼‰æ‡‰æœ‰2è¡Œè³‡æ–™ï¼Œä»£è¡¨å…©å€‹å¯èƒ½çš„çµæœã€‚")
        f.write("åˆªé™¤åªæœ‰1è¡Œçš„ä¸å®Œæ•´å ´æ™¯ã€‚\n\n")
        
        step4 = next((s for s in report['steps'] if s['step'] == 'check_completeness'), None)
        if step4:
            f.write(f"- **åˆªé™¤**: {step4['removed']:,} è¡Œä¸å®Œæ•´å ´æ™¯ ({step4['removed_pct']})\n")
            f.write(f"- **å‰©é¤˜**: {step4['remaining']:,} è¡Œ\n")
            if 'complete_scenarios' in step4:
                f.write(f"- **å®Œæ•´å ´æ™¯æ•¸**: {step4['complete_scenarios']:,}\n\n")
        
        f.write("#### Step 5: åˆä½µæ–‡åŒ–åœˆåˆ†é¡\n\n")
        f.write("å°‡ `country_cluster_map.csv` çš„æ–‡åŒ–åœˆè³‡è¨Šåˆä½µåˆ°ä¸»è³‡æ–™ï¼Œ")
        f.write("ä¾¿æ–¼å¾ŒçºŒè·¨æ–‡åŒ–æ¯”è¼ƒåˆ†æã€‚\n\n")
        
        # æ–‡åŒ–åœˆåˆ†ä½ˆ
        if 'Cluster' in cleaned_df.columns:
            cluster_dist = cleaned_df['Cluster'].value_counts().sort_index()
            f.write("**æ–‡åŒ–åœˆåˆ†ä½ˆ**:\n\n")
            cluster_names = {-1: 'Unclassified (æœªåˆ†é¡å°åœ‹)', 0: 'Western', 1: 'Eastern', 2: 'Southern'}
            for cluster, count in cluster_dist.items():
                if pd.notna(cluster):
                    cluster_name = cluster_names.get(int(cluster), f'Cluster {int(cluster)}')
                    pct = count / len(cleaned_df) * 100
                    f.write(f"- {cluster_name}: {count:,} è¡Œ ({pct:.1f}%)\n")
        
        # æœ€çµ‚çµæœ
        f.write("\n### æœ€çµ‚çµæœ\n\n")
        final = report['steps'][-1]['remaining']
        retention = (final / original * 100) if original > 0 else 0
        
        f.write(f"- **åŸå§‹è³‡æ–™**: {original:,} è¡Œ\n")
        f.write(f"- **æ¸…ç†å¾Œ**: {final:,} è¡Œ\n")
        f.write(f"- **ä¿ç•™æ¯”ä¾‹**: {retention:.2f}%\n\n")
        
        f.write("æ¸…ç†å¾Œçš„è³‡æ–™èšç„¦æ–¼ã€Œå®ˆæ³•vs.æ•ˆç›Šã€è¡çªæƒ…å¢ƒï¼Œ")
        f.write("å¯é€²è¡Œå¾ŒçºŒçš„æè¿°æ€§åˆ†æã€çµ±è¨ˆæ¨è«–èˆ‡é æ¸¬å»ºæ¨¡ã€‚\n\n")
    
    print(f"âœ… Markdownå ±å‘Š: {report_file}")

def analyze_cleaned_data(df: pd.DataFrame, output_dir: str = 'outputs/tables/chapter2'):
    """
    åˆ†ææ¸…ç†å¾Œçš„è³‡æ–™ç‰¹æ€§
    
    Parameters:
    -----------
    df : pd.DataFrame
        æ¸…ç†å¾Œçš„è³‡æ–™
    output_dir : str
        è¼¸å‡ºç›®éŒ„
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("\nåˆ†ææ¸…ç†å¾Œçš„è³‡æ–™...")
    
    # 1. åœ‹å®¶åˆ†ä½ˆ
    if 'UserCountry3' in df.columns:
        country_dist = df['UserCountry3'].value_counts().head(20).reset_index()
        country_dist.columns = ['åœ‹å®¶ä»£ç¢¼', 'æ±ºç­–æ•¸é‡']
        country_dist['æ¯”ä¾‹'] = (country_dist['æ±ºç­–æ•¸é‡'] / len(df) * 100).round(2).astype(str) + '%'
        
        country_file = output_path / 'cleaned_top20_countries.csv'
        country_dist.to_csv(country_file, index=False, encoding='utf-8-sig')
        print(f"âœ… å‰20ååœ‹å®¶åˆ†ä½ˆ: {country_file}")
    
    # 2. å ´æ™¯æ•¸é‡çµ±è¨ˆ
    if 'ResponseID' in df.columns:
        n_scenarios = df['ResponseID'].nunique()
        n_users = df['UserID'].nunique() if 'UserID' in df.columns else 'N/A'
        
        scenario_stats = pd.DataFrame([{
            'æŒ‡æ¨™': 'å®Œæ•´å ´æ™¯æ•¸',
            'æ•¸å€¼': f"{n_scenarios:,}"
        }, {
            'æŒ‡æ¨™': 'ä½¿ç”¨è€…æ•¸',
            'æ•¸å€¼': f"{n_users:,}" if isinstance(n_users, int) else n_users
        }, {
            'æŒ‡æ¨™': 'å¹³å‡æ¯å ´æ™¯è³‡æ–™è¡Œæ•¸',
            'æ•¸å€¼': f"{len(df) / n_scenarios:.2f}" if n_scenarios > 0 else 'N/A'
        }])
        
        stats_file = output_path / 'cleaned_data_stats.csv'
        scenario_stats.to_csv(stats_file, index=False, encoding='utf-8-sig')
        print(f"âœ… åŸºæœ¬çµ±è¨ˆ: {stats_file}")

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ MIT Moral Machine - è³‡æ–™æ¸…ç† (Step 02)")
    print("=" * 60)
    
    # è¨­å®šæª”æ¡ˆæ—¥èªŒ
    logger = setup_file_logger()
    logger.info("é–‹å§‹åŸ·è¡Œè³‡æ–™æ¸…ç†è…³æœ¬...")
    
    try:
        # Step 1: è¼‰å…¥è³‡æ–™
        print("\nã€Step 1ã€‘è¼‰å…¥è³‡æ–™...")
        loader = DataLoader(data_dir='data/raw')
        
        # æª¢æŸ¥æª”æ¡ˆ
        files_status = loader.check_files_exist()
        if not all(files_status.values()):
            print("\nâŒ éŒ¯èª¤: éƒ¨åˆ†è³‡æ–™æª”æ¡ˆç¼ºå¤±")
            return
        
        # è¼‰å…¥å¿…è¦çš„è³‡æ–™
        print("\nè¼‰å…¥å•å·è³‡æ–™...")
        survey_df = loader.load_survey_data(nrows=None)
        
        print("\nè¼‰å…¥æ–‡åŒ–åœˆåˆ†é¡...")
        cluster_map_df = loader.load_cluster_map()
        
        # Step 2: æ¸…ç†è³‡æ–™
        print("\nã€Step 2ã€‘æ¸…ç†è³‡æ–™...")
        cleaner = DataCleaner()
        cleaned_df = cleaner.clean_data(survey_df, cluster_map_df)
        
        # Step 3: å„²å­˜æ¸…ç†å¾Œçš„è³‡æ–™
        print("\nã€Step 3ã€‘å„²å­˜æ¸…ç†å¾Œçš„è³‡æ–™...")
        save_cleaned_data(cleaned_df)
        
        # Step 4: ç”Ÿæˆå ±å‘Š
        print("\nã€Step 4ã€‘ç”Ÿæˆæ¸…ç†å ±å‘Š...")
        report = cleaner.get_cleaning_report()
        generate_cleaning_report(report)
        generate_markdown_report(report, cleaned_df)
        analyze_cleaned_data(cleaned_df)
        
        # å®Œæˆ
        print("\n" + "=" * 60)
        print("âœ… è³‡æ–™æ¸…ç†å®Œæˆï¼")
        print("=" * 60)
        print("\nğŸ“Š å·²ç”¢ç”Ÿä»¥ä¸‹è¼¸å‡º:")
        print("  - data/processed/cleaned_survey.csv")
        print("  - outputs/logs/data_cleaning.log")
        print("  - outputs/tables/chapter2/cleaning_steps_summary.csv")
        print("  - outputs/tables/chapter2/cleaning_report.json")
        print("  - outputs/tables/chapter2/cleaned_top20_countries.csv")
        print("  - outputs/tables/chapter2/cleaned_data_stats.csv")
        print("  - report/drafts/chapter2_section2_data_cleaning.md")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥: python scripts/03_feature_engineering.py")
        print("=" * 60 + "\n")
        
        logger.info("è³‡æ–™æ¸…ç†è…³æœ¬åŸ·è¡Œå®Œæˆ")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        print(f"\nâŒ éŒ¯èª¤: {e}")
        raise

if __name__ == '__main__':
    main()