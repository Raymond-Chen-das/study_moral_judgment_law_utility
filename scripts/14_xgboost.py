"""
ç¬¬5ç«  ç¬¬2ç¯€ï¼šXGBoost æ©Ÿå™¨å­¸ç¿’é©—è­‰
==================================
ç›®æ¨™ï¼šä»¥æ©Ÿå™¨å­¸ç¿’æ–¹æ³•é©—è­‰ç¬¬4ç« çµ±è¨ˆæ¨è«–çš„ç™¼ç¾

åŸ·è¡Œæ­¤è…³æœ¬å‰è«‹ç¢ºèªï¼š
1. å·²å®Œæˆç‰¹å¾µå·¥ç¨‹ï¼Œç”¢å‡º train_data.csv å’Œ test_data.csv
2. è³‡æ–™ä½æ–¼ data/processed/

ç”¢å‡ºï¼š
- outputs/figures/chapter5/roc_curve.html
- outputs/figures/chapter5/confusion_matrix.html
- outputs/figures/chapter5/shap_importance.html
- outputs/tables/chapter5/model_performance.csv
- outputs/tables/chapter5/shap_feature_importance.csv
- outputs/models/xgboost_model.pkl
- report/drafts/chapter5_section2_xgboost.md
"""

import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd
import numpy as np

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from src.modeling.data_transformer import (
    SceneLevelTransformer,
    prepare_features_for_xgboost
)
from src.modeling.xgboost_classifier import (
    MoralChoiceXGBoostClassifier,
    create_performance_summary
)
from src.modeling.shap_analyzer import (
    SHAPAnalyzer,
    prepare_chapter4_comparison_data
)
from src.visualization.chapter5.chapter5_plots import (
    plot_roc_curve,
    plot_confusion_matrix,
    plot_shap_importance,
    plot_shap_vs_chapter4
)


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    
    print("=" * 70)
    print("ç¬¬5ç«  ç¬¬2ç¯€ï¼šXGBoost æ©Ÿå™¨å­¸ç¿’é©—è­‰")
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # ========================================
    # 1. è¨­å®šè·¯å¾‘
    # ========================================
    
    # è¼¸å…¥è·¯å¾‘
    TRAIN_PATH = PROJECT_ROOT / "data/processed/train_data.csv"
    TEST_PATH = PROJECT_ROOT / "data/processed/test_data.csv"
    
    # è¼¸å‡ºè·¯å¾‘
    OUTPUT_FIG_DIR = PROJECT_ROOT / "outputs/figures/chapter5"
    OUTPUT_TABLE_DIR = PROJECT_ROOT / "outputs/tables/chapter5"
    OUTPUT_MODEL_DIR = PROJECT_ROOT / "outputs/models"
    REPORT_DIR = PROJECT_ROOT / "report/drafts"
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    OUTPUT_FIG_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_TABLE_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_MODEL_DIR.mkdir(parents=True, exist_ok=True)
    REPORT_DIR.mkdir(parents=True, exist_ok=True)
    
    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
    if not TRAIN_PATH.exists() or not TEST_PATH.exists():
        print(f"\nâŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¨“ç·´/æ¸¬è©¦è³‡æ–™")
        print(f"   è¨“ç·´é›†: {TRAIN_PATH} - {'å­˜åœ¨' if TRAIN_PATH.exists() else 'ä¸å­˜åœ¨'}")
        print(f"   æ¸¬è©¦é›†: {TEST_PATH} - {'å­˜åœ¨' if TEST_PATH.exists() else 'ä¸å­˜åœ¨'}")
        print("   è«‹å…ˆåŸ·è¡Œç‰¹å¾µå·¥ç¨‹è…³æœ¬")
        return
    
    print(f"\nâœ… è¼¸å…¥æª”æ¡ˆç¢ºèªå®Œæˆ")
    
    # ========================================
    # 2. è³‡æ–™è¼‰å…¥èˆ‡è½‰æ›
    # ========================================
    
    print("\n" + "=" * 60)
    print("è¼‰å…¥èˆ‡è½‰æ›è³‡æ–™")
    print("=" * 60)
    
    # è¼‰å…¥è³‡æ–™
    print("\nè¼‰å…¥è¨“ç·´é›†...")
    train_raw = pd.read_csv(TRAIN_PATH)
    print(f"  åŸå§‹è¡Œæ•¸: {len(train_raw):,}")
    
    print("\nè¼‰å…¥æ¸¬è©¦é›†...")
    test_raw = pd.read_csv(TEST_PATH)
    print(f"  åŸå§‹è¡Œæ•¸: {len(test_raw):,}")
    
    # è½‰æ›ç‚ºå ´æ™¯å±¤ç´š
    transformer = SceneLevelTransformer(verbose=True)
    
    print("\nè½‰æ›è¨“ç·´é›†...")
    train_scene = transformer.transform(train_raw, exclude_unclassified=True)
    
    print("\nè½‰æ›æ¸¬è©¦é›†...")
    test_scene = transformer.transform(test_raw, exclude_unclassified=True)
    
    # ========================================
    # 3. ç‰¹å¾µæº–å‚™
    # ========================================
    
    print("\n" + "=" * 60)
    print("ç‰¹å¾µæº–å‚™")
    print("=" * 60)
    
    # å®šç¾©ç‰¹å¾µ
    FEATURE_COLS = [
        # å ´æ™¯çµæ§‹
        'DiffNumberOFCharacters',
        'PedPed',
        # ä½¿ç”¨è€…ç‰¹å¾µ
        'Review_age',
        'Review_political',
        'Review_religious',
        # æ–‡åŒ–åœˆ
        'Cluster',
        # åœ‹å®¶å±¤ç´š
        'country_law_preference',
        'country_utilitarian',
    ]
    
    # æ·»åŠ  Intervention ç‰¹å¾µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'lawful_requires_intervention' in train_scene.columns:
        FEATURE_COLS.append('lawful_requires_intervention')
    
    TARGET_COL = 'chose_lawful'
    
    # åˆ†é›¢ç‰¹å¾µèˆ‡ç›®æ¨™
    X_train, y_train = transformer.get_feature_target_split(
        train_scene, target_col=TARGET_COL, feature_cols=FEATURE_COLS
    )
    X_test, y_test = transformer.get_feature_target_split(
        test_scene, target_col=TARGET_COL, feature_cols=FEATURE_COLS
    )
    
    # è™•ç†ç¼ºå¤±å€¼ï¼ˆç°¡å–®ç­–ç•¥ï¼šå¡«è£œä¸­ä½æ•¸ï¼‰
    print("\nè™•ç†ç¼ºå¤±å€¼...")
    for col in X_train.columns:
        if X_train[col].isna().any():
            median_val = X_train[col].median()
            X_train[col] = X_train[col].fillna(median_val)
            X_test[col] = X_test[col].fillna(median_val)
            print(f"  {col}: ä»¥ä¸­ä½æ•¸ {median_val:.2f} å¡«è£œ")
    
    # One-Hot ç·¨ç¢¼
    print("\né€²è¡Œ One-Hot ç·¨ç¢¼...")
    X_train_processed = prepare_features_for_xgboost(X_train, cluster_onehot=True)
    X_test_processed = prepare_features_for_xgboost(X_test, cluster_onehot=True)
    
    print(f"\næœ€çµ‚ç‰¹å¾µæ•¸: {X_train_processed.shape[1]}")
    print(f"ç‰¹å¾µåˆ—è¡¨: {list(X_train_processed.columns)}")
    
    # ========================================
    # 4. æ¨¡å‹è¨“ç·´
    # ========================================
    
    print("\n" + "=" * 60)
    print("XGBoost æ¨¡å‹è¨“ç·´")
    print("=" * 60)
    
    # åˆå§‹åŒ–åˆ†é¡å™¨
    classifier = MoralChoiceXGBoostClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbose=True
    )
    
    # è¨“ç·´
    classifier.fit(X_train_processed, y_train)
    
    # ========================================
    # 5. æ¨¡å‹è©•ä¼°
    # ========================================
    
    print("\n" + "=" * 60)
    print("æ¨¡å‹è©•ä¼°")
    print("=" * 60)
    
    metrics = classifier.evaluate(X_test_processed, y_test, return_predictions=True)
    
    # äº¤å‰é©—è­‰
    cv_results = classifier.cross_validate(X_train_processed, y_train, cv=5)
    
    # ========================================
    # 6. SHAP åˆ†æ
    # ========================================
    
    print("\n" + "=" * 60)
    print("SHAP å¯è§£é‡‹æ€§åˆ†æ")
    print("=" * 60)
    
    # åˆå§‹åŒ– SHAP åˆ†æå™¨
    shap_analyzer = SHAPAnalyzer(
        model=classifier.model,
        feature_names=list(X_train_processed.columns),
        verbose=True
    )
    
    # è¨ˆç®— SHAP å€¼ï¼ˆæŠ½æ¨£ä»¥åŠ é€Ÿï¼‰
    shap_sample_size = min(10000, len(X_test_processed))
    shap_analyzer.compute_shap_values(X_test_processed, sample_size=shap_sample_size)
    
    # ç²å–ç‰¹å¾µé‡è¦æ€§
    shap_importance = shap_analyzer.get_feature_importance()
    
    # èˆ‡ç¬¬4ç« æ¯”è¼ƒ
    chapter4_effects = prepare_chapter4_comparison_data()
    comparison_df = shap_analyzer.compare_with_chapter4(chapter4_effects)
    
    # ========================================
    # 7. è¦–è¦ºåŒ–
    # ========================================
    
    print("\n" + "=" * 60)
    print("ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨")
    print("=" * 60)
    
    # ROC æ›²ç·š
    fig_roc = plot_roc_curve(
        metrics=metrics,
        output_path=str(OUTPUT_FIG_DIR / "roc_curve.html"),
        title="XGBoost æ¨¡å‹ ROC æ›²ç·š"
    )
    
    # æ··æ·†çŸ©é™£
    fig_cm = plot_confusion_matrix(
        metrics=metrics,
        output_path=str(OUTPUT_FIG_DIR / "confusion_matrix.html"),
        title="XGBoost æ¨¡å‹æ··æ·†çŸ©é™£"
    )
    
    # SHAP é‡è¦æ€§
    fig_shap = plot_shap_importance(
        importance_df=shap_importance,
        output_path=str(OUTPUT_FIG_DIR / "shap_importance.html"),
        title="SHAP ç‰¹å¾µé‡è¦æ€§",
        top_n=len(shap_importance)
    )
    
    # ========================================
    # 8. å„²å­˜çµæœ
    # ========================================
    
    print("\n" + "=" * 60)
    print("å„²å­˜åˆ†æçµæœ")
    print("=" * 60)
    
    # æ€§èƒ½æŒ‡æ¨™
    performance_df = pd.DataFrame([{
        'metric': 'Accuracy',
        'value': metrics['accuracy'],
        'cv_mean': cv_results.get('accuracy_mean', np.nan),
        'cv_std': cv_results.get('accuracy_std', np.nan)
    }, {
        'metric': 'Precision',
        'value': metrics['precision'],
        'cv_mean': cv_results.get('precision_mean', np.nan),
        'cv_std': cv_results.get('precision_std', np.nan)
    }, {
        'metric': 'Recall',
        'value': metrics['recall'],
        'cv_mean': cv_results.get('recall_mean', np.nan),
        'cv_std': cv_results.get('recall_std', np.nan)
    }, {
        'metric': 'F1 Score',
        'value': metrics['f1'],
        'cv_mean': cv_results.get('f1_mean', np.nan),
        'cv_std': cv_results.get('f1_std', np.nan)
    }, {
        'metric': 'ROC-AUC',
        'value': metrics['roc_auc'],
        'cv_mean': cv_results.get('roc_auc_mean', np.nan),
        'cv_std': cv_results.get('roc_auc_std', np.nan)
    }])
    
    performance_df.to_csv(
        OUTPUT_TABLE_DIR / "model_performance.csv",
        index=False,
        encoding='utf-8-sig'
    )
    print(f"âœ… å·²å„²å­˜: model_performance.csv")
    
    # SHAP é‡è¦æ€§
    shap_importance.to_csv(
        OUTPUT_TABLE_DIR / "shap_feature_importance.csv",
        index=False,
        encoding='utf-8-sig'
    )
    print(f"âœ… å·²å„²å­˜: shap_feature_importance.csv")
    
    # æ¨¡å‹
    classifier.save_model(str(OUTPUT_MODEL_DIR / "xgboost_model.pkl"))
    
    # ========================================
    # 9. ç”Ÿæˆå ±å‘Šè‰ç¨¿
    # ========================================
    
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå ±å‘Šè‰ç¨¿")
    print("=" * 60)
    
    report_content = generate_section_report(
        metrics=metrics,
        cv_results=cv_results,
        shap_importance=shap_importance,
        X_train=X_train_processed,
        y_train=y_train
    )
    
    report_path = REPORT_DIR / "chapter5_section2_xgboost.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    print(f"âœ… å·²å„²å­˜: {report_path}")
    
    # ========================================
    # 10. ç¸½çµ
    # ========================================
    
    print("\n" + "=" * 70)
    print("ç¬¬5.2ç¯€åŸ·è¡Œå®Œæˆï¼")
    print("=" * 70)
    
    print("\nğŸ“Š ç”¢å‡ºæª”æ¡ˆï¼š")
    print(f"   - {OUTPUT_FIG_DIR / 'roc_curve.html'}")
    print(f"   - {OUTPUT_FIG_DIR / 'confusion_matrix.html'}")
    print(f"   - {OUTPUT_FIG_DIR / 'shap_importance.html'}")
    print(f"   - {OUTPUT_TABLE_DIR / 'model_performance.csv'}")
    print(f"   - {OUTPUT_TABLE_DIR / 'shap_feature_importance.csv'}")
    print(f"   - {OUTPUT_MODEL_DIR / 'xgboost_model.pkl'}")
    
    print("\nğŸ”‘ é—œéµç™¼ç¾ï¼š")
    print(f"   - Accuracy: {metrics['accuracy']:.4f}")
    print(f"   - ROC-AUC: {metrics['roc_auc']:.4f}")
    print(f"   - æœ€é‡è¦ç‰¹å¾µ: {shap_importance.iloc[0]['feature']}")
    print(f"     (SHAP = {shap_importance.iloc[0]['shap_importance']:.4f})")


def generate_section_report(
    metrics: dict,
    cv_results: dict,
    shap_importance: pd.DataFrame,
    X_train: pd.DataFrame,
    y_train: pd.Series
) -> str:
    """ç”Ÿæˆ 5.2 ç¯€å ±å‘Šè‰ç¨¿"""
    
    report = []
    report.append("## 5.2 æ©Ÿå™¨å­¸ç¿’é©—è­‰\n")
    report.append(f"**åˆ†ææ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    report.append("### ç ”ç©¶ç›®çš„\n")
    report.append("ä»¥ XGBoost æ©Ÿå™¨å­¸ç¿’æ¨¡å‹é©—è­‰ç¬¬4ç« çµ±è¨ˆæ¨è«–çš„ç™¼ç¾ï¼Œ")
    report.append("ä¸¦é€é SHAP å¯è§£é‡‹æ€§åˆ†ææ¯”è¼ƒç‰¹å¾µé‡è¦æ€§æ’åºã€‚\n")
    
    report.append("### è³‡æ–™èˆ‡ç‰¹å¾µ\n")
    report.append(f"- **è¨“ç·´æ¨£æœ¬æ•¸**: {len(X_train):,}")
    report.append(f"- **ç‰¹å¾µæ•¸**: {X_train.shape[1]}")
    report.append(f"- **ç›®æ¨™è®Šæ•¸åˆ†ä½ˆ**: chose_lawful=1 ä½” {y_train.mean():.1%}\n")
    
    report.append("**ç‰¹å¾µåˆ—è¡¨**ï¼š")
    for col in X_train.columns:
        report.append(f"- {col}")
    report.append("")
    
    report.append("### æ¨¡å‹æ€§èƒ½\n")
    report.append("| æŒ‡æ¨™ | æ¸¬è©¦é›† | 5-fold CV |")
    report.append("|------|--------|-----------|")
    report.append(f"| Accuracy | {metrics['accuracy']:.4f} | {cv_results.get('accuracy_mean', 0):.4f} Â± {cv_results.get('accuracy_std', 0):.4f} |")
    report.append(f"| Precision | {metrics['precision']:.4f} | - |")
    report.append(f"| Recall | {metrics['recall']:.4f} | - |")
    report.append(f"| F1 Score | {metrics['f1']:.4f} | - |")
    report.append(f"| ROC-AUC | {metrics['roc_auc']:.4f} | {cv_results.get('roc_auc_mean', 0):.4f} Â± {cv_results.get('roc_auc_std', 0):.4f} |")
    report.append("")
    
    report.append("### SHAP ç‰¹å¾µé‡è¦æ€§\n")
    report.append("| æ’åº | ç‰¹å¾µ | SHAP é‡è¦æ€§ | å½±éŸ¿æ–¹å‘ |")
    report.append("|------|------|------------|----------|")
    for i, row in shap_importance.head(10).iterrows():
        report.append(f"| {row['rank']} | {row['feature']} | {row['shap_importance']:.4f} | {row['direction']} |")
    report.append("")
    
    report.append("### èˆ‡ç¬¬4ç« çš„æ¯”è¼ƒ\n")
    report.append("**ä¸€è‡´æ€§é©—è­‰**ï¼š")
    report.append(f"- æ¨¡å‹ ROC-AUC = {metrics['roc_auc']:.4f}ï¼Œé¡¯ç¤ºé æ¸¬èƒ½åŠ›æœ‰é™")
    report.append("- èˆ‡ç¬¬4ç«  Pseudo RÂ² = 0.0004 çš„ç™¼ç¾ä¸€è‡´ï¼šå€‹äºº/æ–‡åŒ–å› ç´ å°é“å¾·é¸æ“‡çš„è§£é‡‹åŠ›æœ‰é™")
    report.append("- SHAP æ’åºèˆ‡ç¬¬4ç« æ•ˆæœé‡æ–¹å‘ä¸€è‡´\n")
    
    report.append("### é—œéµç™¼ç¾\n")
    top_feat = shap_importance.iloc[0]
    report.append(f"1. **æœ€é‡è¦ç‰¹å¾µ**ï¼š{top_feat['feature']} (SHAP = {top_feat['shap_importance']:.4f})")
    report.append(f"2. **é æ¸¬èƒ½åŠ›æœ‰é™**ï¼šAUC = {metrics['roc_auc']:.4f}ï¼Œç•¥å„ªæ–¼éš¨æ©ŸçŒœæ¸¬")
    report.append("3. **é©—è­‰æƒ…å¢ƒä¸»ç¾©**ï¼šå³ä½¿ä½¿ç”¨éç·šæ€§æ¨¡å‹ï¼Œå€‹äºº/æ–‡åŒ–å› ç´ ä»é›£ä»¥é æ¸¬é“å¾·é¸æ“‡\n")
    
    report.append("### è¦–è¦ºåŒ–çµæœ\n")
    report.append("- [ROC æ›²ç·š](../outputs/figures/chapter5/roc_curve.html)")
    report.append("- [æ··æ·†çŸ©é™£](../outputs/figures/chapter5/confusion_matrix.html)")
    report.append("- [SHAP ç‰¹å¾µé‡è¦æ€§](../outputs/figures/chapter5/shap_importance.html)\n")
    
    return "\n".join(report)


if __name__ == "__main__":
    main()