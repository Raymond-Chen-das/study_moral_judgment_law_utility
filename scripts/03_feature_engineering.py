"""
03_feature_engineering.py
==========================
ç¬¬ä¸‰æ­¥ï¼šç‰¹å¾µå·¥ç¨‹

åŠŸèƒ½ï¼š
1. å»ºç«‹å ´æ™¯å±¤ç´šç‰¹å¾µï¼ˆå®ˆæ³•ã€å¤šæ•¸ã€è¡çªç­‰ï¼‰
2. âš ï¸ é—œéµéæ¿¾ï¼šåªä¿ç•™ã€Œå®ˆæ³• vs. æ•ˆç›Šã€è¡çªå ´æ™¯
3. å»ºç«‹ä½¿ç”¨è€…é“å¾·å´å¯«
4. åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
5. ç”¢ç”Ÿç‰¹å¾µèªªæ˜æ–‡ä»¶

åŸ·è¡Œæ–¹å¼ï¼š
    python scripts/03_feature_engineering.py
"""

import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥è·¯å¾‘
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.data.feature_engineer import FeatureEngineer
import pandas as pd
import logging
from datetime import datetime
import json

def setup_file_logger(log_dir: str = 'outputs/logs') -> logging.Logger:
    """è¨­å®šæª”æ¡ˆæ—¥èªŒè¨˜éŒ„å™¨"""
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    log_file = log_path / 'feature_engineering.log'
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    
    logger = logging.getLogger()
    logger.addHandler(file_handler)
    logger.setLevel(logging.INFO)
    
    return logger

def save_featured_data(df: pd.DataFrame, output_dir: str = 'data/processed'):
    """å„²å­˜å¢åŠ ç‰¹å¾µçš„è³‡æ–™"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    output_file = output_path / 'featured_data.csv'
    
    print(f"\nå„²å­˜ç‰¹å¾µåŒ–è³‡æ–™...")
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    file_size_mb = output_file.stat().st_size / 1024**2
    print(f"âœ… å·²å„²å­˜: {output_file}")
    print(f"   æª”æ¡ˆå¤§å°: {file_size_mb:.2f} MB")
    print(f"   æ¬„ä½æ•¸: {len(df.columns)}")

def save_user_profiles(profiles_df: pd.DataFrame, output_dir: str = 'data/processed'):
    """å„²å­˜ä½¿ç”¨è€…é“å¾·å´å¯«"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    output_file = output_path / 'user_moral_profiles.csv'
    
    print(f"\nå„²å­˜ä½¿ç”¨è€…é“å¾·å´å¯«...")
    profiles_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    print(f"âœ… å·²å„²å­˜: {output_file}")
    print(f"   ä½¿ç”¨è€…æ•¸: {len(profiles_df):,}")

def save_train_test_split(train_df: pd.DataFrame, 
                          test_df: pd.DataFrame,
                          output_dir: str = 'data/processed'):
    """å„²å­˜è¨“ç·´/æ¸¬è©¦é›†"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nå„²å­˜è¨“ç·´/æ¸¬è©¦é›†...")
    
    # å„²å­˜è¨“ç·´é›†
    train_file = output_path / 'train_data.csv'
    train_df.to_csv(train_file, index=False, encoding='utf-8-sig')
    print(f"âœ… è¨“ç·´é›†: {train_file}")
    print(f"   {len(train_df):,} è¡Œ")
    
    # å„²å­˜æ¸¬è©¦é›†
    test_file = output_path / 'test_data.csv'
    test_df.to_csv(test_file, index=False, encoding='utf-8-sig')
    print(f"âœ… æ¸¬è©¦é›†: {test_file}")
    print(f"   {len(test_df):,} è¡Œ")
    
    # å„²å­˜åˆ†å‰²ç´¢å¼•
    split_index = {
        'train_users': train_df['UserID'].unique().tolist(),
        'test_users': test_df['UserID'].unique().tolist(),
        'split_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'dataset_type': 'conflict_only'
    }
    
    index_file = output_path / 'train_test_split.json'
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(split_index, f, ensure_ascii=False, indent=2)

def save_feature_descriptions(descriptions: dict, output_dir: str = 'outputs/tables/chapter2'):
    """å„²å­˜ç‰¹å¾µèªªæ˜æ–‡ä»¶"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    desc_df = pd.DataFrame([
        {'ç‰¹å¾µåç¨±': name, 'èªªæ˜': desc}
        for name, desc in descriptions.items()
    ])
    
    csv_file = output_path / 'feature_descriptions.csv'
    desc_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ… ç‰¹å¾µèªªæ˜æ–‡ä»¶: {csv_file}")

def generate_feature_statistics(df: pd.DataFrame, output_dir: str = 'outputs/tables/chapter2'):
    """ç”Ÿæˆç‰¹å¾µçµ±è¨ˆå ±å‘Š"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    scenario_stats = []
    feature_cols = ['is_lawful', 'is_majority', 'chose_lawful', 
                   'chose_majority', 'lawful_vs_majority_conflict']
    
    for col in feature_cols:
        if col in df.columns:
            scenario_stats.append({
                'ç‰¹å¾µ': col,
                'å¹³å‡å€¼': f"{df[col].mean():.3f}",
                'æ¨™æº–å·®': f"{df[col].std():.3f}",
                'æœ€å°å€¼': int(df[col].min()),
                'æœ€å¤§å€¼': int(df[col].max()),
                'ç¸½å’Œ': f"{df[col].sum():,}"
            })
    
    stats_df = pd.DataFrame(scenario_stats)
    stats_file = output_path / 'scenario_feature_stats.csv'
    stats_df.to_csv(stats_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å ´æ™¯ç‰¹å¾µçµ±è¨ˆ: {stats_file}")

def generate_markdown_report(df: pd.DataFrame,
                            profiles_df: pd.DataFrame,
                            descriptions: dict,
                            original_count: int,
                            output_dir: str = 'report/drafts'):
    """ç”ŸæˆMarkdownæ ¼å¼çš„å ±å‘Šè‰ç¨¿"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    report_file = output_path / 'chapter2_section3_feature_engineering.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç¬¬2ç«  è³‡æ–™è™•ç†\n\n")
        f.write("## 2.3 ç‰¹å¾µå·¥ç¨‹èˆ‡å ´æ™¯ç¯©é¸\n\n")
        
        f.write("### æ ¸å¿ƒç›®æ¨™\n\n")
        f.write("æœ¬æ­¥é©Ÿå°‡è³‡æ–™**åš´æ ¼é™ç¸®æ–¼ã€Œé“å¾·å…©é›£ã€æƒ…å¢ƒ**ï¼Œä¸¦å»ºç«‹ç›¸é—œç‰¹å¾µã€‚\n")
        f.write("ç‚ºäº†ç¢ºä¿å¾ŒçºŒåˆ†æèˆ‡æ¨¡å‹é æ¸¬çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘å€‘ç§»é™¤äº†æ‰€æœ‰ã€Œéè¡çªã€å ´æ™¯ï¼ˆå³å®ˆæ³•èˆ‡æ•‘å¤šæ•¸ä¸€è‡´çš„é€åˆ†é¡Œï¼‰ã€‚\n\n")
        
        f.write("### è³‡æ–™éæ¿¾çµæœ\n\n")
        filtered_count = len(df)
        removed_count = original_count - filtered_count
        
        f.write(f"- **åŸå§‹è³‡æ–™é‡**: {original_count:,} è¡Œ\n")
        f.write(f"- **éæ¿¾å¾Œè³‡æ–™é‡**: {filtered_count:,} è¡Œ (åƒ…åŒ…å«è¡çªå ´æ™¯)\n")
        f.write(f"- **ç§»é™¤è³‡æ–™é‡**: {removed_count:,} è¡Œ ({removed_count/original_count*100:.1f}%)\n")
        f.write("- **ç¯©é¸æ¨™æº–**: `lawful_vs_majority_conflict == 1`\n\n")
        
        f.write("### å ´æ™¯å±¤ç´šç‰¹å¾µçµ±è¨ˆ\n\n")
        f.write("ç”±æ–¼è³‡æ–™å·²ç¯©é¸ç‚ºè¡çªå ´æ™¯ï¼Œ`lawful_vs_majority_conflict` çš„å¹³å‡å€¼ç‚º 1.0ã€‚\n\n")
        f.write("| ç‰¹å¾µåç¨± | èªªæ˜ | å¹³å‡å€¼ |\n")
        f.write("|---------|------|--------|\n")
        
        scenario_features = ['is_lawful', 'is_majority', 'chose_lawful', 'chose_majority']
        
        for feat in scenario_features:
            if feat in df.columns:
                desc = descriptions.get(feat, '')
                mean_val = df[feat].mean()
                f.write(f"| {feat} | {desc} | {mean_val:.3f} |\n")
        
        f.write("\n#### é—œéµç™¼ç¾\n\n")
        if 'chose_lawful' in df.columns and 'Saved' in df.columns:
            chose_lawful_rate = df[df['Saved'] == 1]['is_lawful'].mean()
            f.write(f"- **çœŸå¯¦å®ˆæ³•é¸æ“‡ç‡**: {chose_lawful_rate*100:.1f}%\n")
            f.write("  ï¼ˆè¨»ï¼šæ­¤æ•¸å€¼åæ˜ åœ¨å¿…é ˆçŠ§ç‰²å¤šæ•¸äººæ™‚ï¼Œé¸æ“‡å®ˆæ³•çš„æ¯”ä¾‹ï¼‰\n")
        
        # åœ‹å®¶å±¤ç´šç‰¹å¾µ
        f.write("\n### åœ‹å®¶å±¤ç´šç‰¹å¾µ\n\n")
        f.write("å·²æ•´åˆ `CountriesChangePr.csv` çš„ AMCE å€¼ï¼Œç”¨æ–¼å¾ŒçºŒéšå±¤æ¨¡å‹åˆ†æã€‚\n")
        
        # ä½¿ç”¨è€…å´å¯«
        f.write("\n### ä½¿ç”¨è€…é“å¾·å´å¯«\n\n")
        f.write(f"- **å´å¯«ä½¿ç”¨è€…æ•¸**: {len(profiles_df):,} ä½\n")
        f.write(f"- **å¹³å‡å®Œæˆè¡çªå ´æ™¯æ•¸**: {profiles_df['n_scenarios'].mean():.1f} å€‹\n\n")
        
        if 'utilitarian_score' in profiles_df.columns:
            strong_util = (profiles_df['utilitarian_score'] > 0.7).sum()
            f.write("**æ•ˆç›Šä¸»ç¾©å‚¾å‘åˆ†ä½ˆ**:\n")
            f.write(f"- å¼·æ•ˆç›Šä¸»ç¾© (>0.7): {strong_util:,} ä½ ({strong_util/len(profiles_df)*100:.1f}%)\n")

        f.write("\n### çµè«–\n\n")
        f.write("æœ¬è³‡æ–™é›†ç¾å·²æº–å‚™å¥½é€²è¡Œåˆ†æã€‚æ‰€æœ‰å¾ŒçºŒç« ç¯€ï¼ˆæ¢ç´¢æ€§åˆ†æã€æ¨è«–çµ±è¨ˆã€é æ¸¬æ¨¡å‹ï¼‰\n")
        f.write("éƒ½å°‡åŸºæ–¼æ­¤ã€Œç´”è¡çªã€è³‡æ–™é›†é€²è¡Œï¼Œç¢ºä¿ç ”ç©¶èšç„¦æ–¼çœŸå¯¦çš„é“å¾·æ¬Šè¡¡ã€‚\n")
    
    print(f"âœ… Markdownå ±å‘Š: {report_file}")

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "=" * 60)
    print("ğŸ”§ MIT Moral Machine - ç‰¹å¾µå·¥ç¨‹ (Step 03)")
    print("=" * 60)
    
    logger = setup_file_logger()
    logger.info("é–‹å§‹åŸ·è¡Œç‰¹å¾µå·¥ç¨‹è…³æœ¬...")
    
    try:
        # Step 1: è¼‰å…¥è³‡æ–™
        print("\nã€Step 1ã€‘è¼‰å…¥è³‡æ–™...")
        cleaned_file = Path('data/processed/cleaned_survey.csv')
        if not cleaned_file.exists():
            print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° {cleaned_file}ï¼Œè«‹å…ˆåŸ·è¡Œ 02_data_cleaning.py")
            return
        
        df = pd.read_csv(cleaned_file)
        print(f"âœ… è¼‰å…¥è³‡æ–™: {len(df):,} è¡Œ")
        
        countries_file = Path('data/raw/CountriesChangePr.csv')
        countries_df = pd.read_csv(countries_file) if countries_file.exists() else None
        
        # Step 2: å»ºç«‹ç‰¹å¾µ
        print("\nã€Step 2ã€‘å»ºç«‹å ´æ™¯ç‰¹å¾µ...")
        engineer = FeatureEngineer()
        df_featured = engineer.engineer_features(df)
        
        # Step 3: åˆä½µåœ‹å®¶ç‰¹å¾µ
        if countries_df is not None:
            print("\nã€Step 3ã€‘åˆä½µåœ‹å®¶ç‰¹å¾µ...")
            df_featured = engineer.merge_country_features(df_featured, countries_df)
            df_featured = engineer.add_feature_availability_flag(df_featured)
        
        # ==========================================
        # ğŸŸ¢ Step 3.5: é—œéµéæ¿¾
        # ==========================================
        print("\nã€Step 3.5ã€‘âš ï¸  å¼·åˆ¶ç¯©é¸ï¼šåªä¿ç•™ã€Œå®ˆæ³•vs.æ•ˆç›Šã€è¡çªå ´æ™¯...")
        n_before = len(df_featured)
        
        if 'lawful_vs_majority_conflict' in df_featured.columns:
            # éæ¿¾è³‡æ–™
            df_featured = df_featured[df_featured['lawful_vs_majority_conflict'] == 1].copy()
            n_after = len(df_featured)
            
            print(f"   éæ¿¾å‰: {n_before:,} è¡Œ")
            print(f"   éæ¿¾å¾Œ: {n_after:,} è¡Œ (å·²ç§»é™¤ {n_before - n_after:,} è¡Œéè¡çªè³‡æ–™)")
            print("   âœ… Dataset ç¾åœ¨åƒ…åŒ…å«çœŸæ­£çš„é“å¾·å…©é›£æƒ…å¢ƒ")
        else:
            print("   âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¡çªæ¨™è¨˜æ¬„ä½ï¼Œç„¡æ³•ç¯©é¸ï¼")
            return
        
        # Step 4: åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
        print("\nã€Step 4ã€‘åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†...")
        train_df, test_df = engineer.split_train_test(df_featured)
        
        # Step 5: å»ºç«‹ä½¿ç”¨è€…å´å¯«
        print("\nã€Step 5ã€‘å»ºç«‹ä½¿ç”¨è€…é“å¾·å´å¯«...")
        train_profiles = engineer.create_user_profiles(train_df)
        train_profiles['split'] = 'train'
        
        test_profiles = engineer.create_user_profiles(test_df)
        test_profiles['split'] = 'test'
        
        all_profiles = pd.concat([train_profiles, test_profiles], ignore_index=True)
        
        # Step 6: å„²å­˜
        print("\nã€Step 6ã€‘å„²å­˜çµæœ...")
        save_featured_data(df_featured)
        save_user_profiles(all_profiles)
        save_train_test_split(train_df, test_df)
        
        # Step 7: å ±å‘Š
        print("\nã€Step 7ã€‘ç”¢ç”Ÿå ±å‘Š...")
        descriptions = engineer.get_feature_descriptions()
        save_feature_descriptions(descriptions)
        generate_feature_statistics(df_featured)
        generate_markdown_report(df_featured, all_profiles, descriptions, n_before)
        
        print("\n" + "=" * 60)
        print("âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼(Conflict-Only Dataset Created)")
        print("=" * 60)
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        print(f"\nâŒ éŒ¯èª¤: {e}")
        raise

if __name__ == '__main__':
    main()